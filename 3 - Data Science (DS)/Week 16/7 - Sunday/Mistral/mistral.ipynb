{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OpenAI --> API with gpt-3.5-turbo, gpt-4o (pricy since we use an API)\n",
    "- Llama --> Open source (by Meta)\n",
    "- Mistral --> Open source (by Mistral AI)\n",
    "\n",
    "- The latter 2 --> free and open source (if downloaded), pay for the API\n",
    "\n",
    "- Mixtral --> mixture of expert (8 models in 1) --> claims to perform equally as good as gpt-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"MISTRAL_API_KEY\"] = \"Nope :)\"\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-large-latest\" # Similar to GPT-3\n",
    "# model = \"open-mistral-7b\"\n",
    "# model = \"open-mixtral-8x22b\"\n",
    "\n",
    "# These open source models could be downloaded for free and hosted by yourself\n",
    "# but if you use the API it would cost a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixtral --> mixture of experts (8 experts) --> 40B parameter model\n",
    "## Query it, calls 2 experts --> 12B parameters for inference\n",
    "\n",
    "# Pros: performance and it's free\n",
    "# Cons: need to host yourself\n",
    "## Host for free --> low latency (slow)\n",
    "## Host on own GPU --> some costs, but less than using AI (in most cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clause 3.5 Sonnet, best model but most expensive (cost lest than GPT-4o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pricing Guide](https://mistral.ai/technology/#pricing) <-- Here you can see how much each model costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MistralClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mistral(user_message, \n",
    "            model=\"open-mixtral-8x22b\",\n",
    "            is_json=False):\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, \n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"})\n",
    "    else:\n",
    "        chat_response = client.chat(\n",
    "            model=model, \n",
    "            messages=messages)\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! As a helpful and harmless assistant, I can provide information, answer questions, offer suggestions, and assist with tasks to the best of my ability. I can help with a wide range of topics, such as general knowledge, science, mathematics, language learning, and more. I can also generate creative content, like stories, poems, and jokes. My primary goal is to make your life easier and more enjoyable. How can I assist you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral(\"Hello, what can you do?\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for you:\n",
      "\n",
      "Why don't we ever tell secrets on a farm?\n",
      "\n",
      "Because the potatoes have eyes, the corn has ears, and the beans stalk.\n"
     ]
    }
   ],
   "source": [
    "response = mistral(\"Tell me a great joke that is not about scientists\", model=model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a bank customer service bot. \n",
    "    Your task is to assess customer intent and categorize customer \n",
    "    inquiry after <<<>>> into one of the following predefined categories:\n",
    "    \n",
    "    card arrival\n",
    "    change pin\n",
    "    exchange rate\n",
    "    country support\n",
    "    cancel transfer\n",
    "    charge dispute\n",
    "    \n",
    "    If the text doesn't fit into any of the above categories, \n",
    "    classify it as:\n",
    "    customer service\n",
    "    \n",
    "    You will only respond with the predefined category. \n",
    "    Do not provide explanations or notes. \n",
    "    \n",
    "    ###\n",
    "    Here are some examples:\n",
    "    \n",
    "    Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
    "    Category: card arrival\n",
    "    Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
    "    Category: exchange rate \n",
    "    Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
    "    Category: country support\n",
    "    Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. \n",
    "    Category: customer service\n",
    "    ###\n",
    "    \n",
    "    <<<\n",
    "    Inquiry: {inquiry}\n",
    "    >>>\n",
    "    Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No examples --> zero-shot prompting\n",
    "\n",
    "## Too many examples --> overfitting in a way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = mistral(f\"Please correct the spelling and grammar of \\\n",
    "this prompt and return a text that is the same prompt,\\\n",
    "with the spelling and grammar fixed: {prompt}\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a bank customer service bot.\n",
      "Your task is to assess customer intent and categorize the customer inquiry after <<<>>> into one of the following predefined categories:\n",
      "\n",
      "card arrival\n",
      "change pin\n",
      "exchange rate\n",
      "country support\n",
      "cancel transfer\n",
      "charge dispute\n",
      "\n",
      "If the text doesn't fit into any of the above categories, classify it as:\n",
      "customer service\n",
      "\n",
      "You will only respond with the predefined category. Do not provide explanations or notes.\n",
      "\n",
      "---\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Inquiry: How do I know if I will receive my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
      "Category: card arrival\n",
      "\n",
      "Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
      "Category: exchange rate\n",
      "\n",
      "Inquiry: What countries are supported? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
      "Category: country support\n",
      "\n",
      "Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue.\n",
      "Category: customer service\n",
      "\n",
      "---\n",
      "\n",
      "<<<\n",
      "Inquiry: {inquiry}\n",
      ">>>\n",
      "Category:\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cancel transfer'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral(\n",
    "    response.format(\n",
    "        inquiry=\"I want to end the transaction I started a while ago\"\n",
    "    ), model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction with JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_notes = \"\"\"\n",
    "A 60-year-old female patient, Mrs. Johnson, presented with symptoms\n",
    "of increased thirst, frequent urination, fatigue, and unexplained\n",
    "weight loss. Upon evaluation, he was diagnosed with diabetes,\n",
    "confirmed by elevated blood sugar levels. Mrs. Johnson's weight\n",
    "is 210 lbs. She has been prescribed Metformin to be taken twice daily\n",
    "with meals. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract information from the following medical notes:\n",
    "{medical_notes}\n",
    "\n",
    "Return json format with the following JSON schema: \n",
    "\n",
    "{{\n",
    "        \"age\": {{\n",
    "            \"type\": \"integer\"\n",
    "        }},\n",
    "        \"gender\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"male\", \"female\", \"other\"]\n",
    "        }},\n",
    "        \"diagnosis\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"migraine\", \"diabetes\", \"arthritis\", \"acne\"]\n",
    "        }},\n",
    "        \"weight\": {{\n",
    "            \"type\": \"integer\"\n",
    "        }},\n",
    "        \"smoking\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"yes\", \"no\", \"undetermined\"]\n",
    "        }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract information from the following medical notes:\n",
    "{medical_notes}\n",
    "\n",
    "###\n",
    "Instructions:\n",
    "\n",
    "1. List the factors that play a role in a person being a smoker.\n",
    "    - Options include \"age\", \"weight\", and \"gender\"\n",
    "2. Based on these factors, predict whether the person is a smoker or not.\n",
    "    - Ouptut one of these words [\"Smoker\", \"Non-smoker\"]\n",
    "3. Give a one-sentence summary to justify the prediction\n",
    "\n",
    "\n",
    "Examples:\n",
    "Medical Notes: Peter is a 12 year old boy, at a healthy weight for his age (80 lbs)\n",
    "- Prediction: Not a smoker, because Peter is a young male that is unlikely to smoke, his weight is not irregular\n",
    "\n",
    "Medical Notes: Sarah is 35 years old, and she wieghts 600 lbs, she has been diagnosed with lung cancer\n",
    "- Prediction: Smoker, because Sarah is quite young with a very high weight and the lung cancer diagnosis may indicate that she is a smoker\n",
    "###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The factors that play a role in a person being a smoker from the given medical notes are age and weight. Gender is also a factor, but it is not explicitly stated in the given information.\n",
      "\n",
      "2. Prediction: Non-smoker\n",
      "\n",
      "3. Mrs. Johnson's symptoms and diagnosis are consistent with diabetes, not smoking-related illnesses, and her age and weight, while contributing to her diabetes risk, do not provide clear evidence for smoking.\n"
     ]
    }
   ],
   "source": [
    "# response = mistral(prompt, model, is_json=True)\n",
    "response = mistral(prompt, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear mortgage lender, \n",
    "\n",
    "What's your 30-year fixed-rate APR, how is it compared to the 15-year \n",
    "fixed rate?\n",
    "\n",
    "Regards,\n",
    "Anna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "You are a mortgage lender customer service bot, and your task is to \n",
    "create personalized email responses to address customer questions.\n",
    "Answer the customer's inquiry using the provided facts below. Ensure \n",
    "that your response is clear, concise, and directly addresses the \n",
    "customer's question. Address the customer in a friendly and \n",
    "professional manner. Sign the email with \"Lender Customer Support.\"   \n",
    "      \n",
    "# Facts\n",
    "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
    "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
    "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
    "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
    "7-year ARM: interest rate 7.011%, APR 7.660%\n",
    "5-year ARM: interest rate 6.880%, APR 7.754%\n",
    "3-year ARM: interest rate 6.125%, APR 7.204%\n",
    "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
    "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
    "\n",
    "# Email\n",
    "{email}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for nomal models\n",
    "# get metrics, we compare test set answers vs our model's predictions\n",
    "# Same for LMs --> compare test with predictions and get accuracy\n",
    "\n",
    "\n",
    "# Test set\n",
    "## Q1, A1\n",
    "## ..., ...\n",
    "## Qn, An\n",
    "\n",
    "# Ask model to predict A for each Q\n",
    "# Compare to get accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression --> LLM + code executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Information on Our 30-Year and 15-Year Fixed-Rate Mortgages\n",
      "\n",
      "Dear Anna,\n",
      "\n",
      "Thank you for reaching out to us with your inquiry. We appreciate your interest in our mortgage offerings.\n",
      "\n",
      "Our current 30-year fixed-rate mortgage comes with an interest rate of 6.403% and an Annual Percentage Rate (APR) of 6.484%. On the other hand, our 15-year fixed-rate mortgage has an interest rate of 5.705% and an APR of 5.848%.\n",
      "\n",
      "When comparing the two, you'll notice that the APR for the 15-year fixed-rate mortgage is lower than that of the 30-year fixed-rate mortgage. This is primarily due to the shorter term of the 15-year mortgage, which allows for a lower interest rate and results in less interest paid over the life of the loan. However, please note that the monthly payments for a 15-year fixed-rate mortgage are typically higher than those for a 30-year fixed-rate mortgage due to the shorter repayment period.\n",
      "\n",
      "We hope this information helps you in making an informed decision. If you have any further questions or need additional clarification, please don't hesitate to contact us.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Lender Customer Support\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsletter = \"\"\"\n",
    "European AI champion Mistral AI unveiled new large language models and formed an alliance with Microsoft. \n",
    "\n",
    "What’s new: Mistral AI introduced two closed models, Mistral Large and Mistral Small (joining Mistral Medium, which debuted quietly late last year). Microsoft invested $16.3 million in the French startup, and it agreed to distribute Mistral Large on its Azure platform and let Mistral AI use Azure computing infrastructure. Mistral AI makes the new models available to try for free here and to use on its La Plateforme and via custom deployments.\n",
    "\n",
    "Model specs: The new models’ parameter counts, architectures, and training methods are undisclosed. Like the earlier, open source Mistral 7B and Mixtral 8x7B, they can process 32,000 tokens of input context. \n",
    "\n",
    "Mistral Large achieved 81.2 percent on the MMLU benchmark, outperforming Anthropic’s Claude 2, Google’s Gemini Pro, and Meta’s Llama 2 70B, though falling short of GPT-4. Mistral Small, which is optimized for latency and cost, achieved 72.2 percent on MMLU.\n",
    "Both models are fluent in French, German, Spanish, and Italian. They’re trained for function calling and JSON-format output.\n",
    "Microsoft’s investment in Mistral AI is significant but tiny compared to its $13 billion stake in OpenAI and Google and Amazon’s investments in Anthropic, which amount to $2 billion and $4 billion respectively.\n",
    "Mistral AI and Microsoft will collaborate to train bespoke models for customers including European governments.\n",
    "Behind the news: Mistral AI was founded in early 2023 by engineers from Google and Meta. The French government has touted the company as a home-grown competitor to U.S.-based leaders like OpenAI. France’s representatives in the European Commission argued on Mistral’s behalf to loosen the European Union’s AI Act oversight on powerful AI models. \n",
    "\n",
    "Yes, but: Mistral AI’s partnership with Microsoft has divided European lawmakers and regulators. The European Commission, which already was investigating Microsoft’s agreement with OpenAI for potential breaches of antitrust law, plans to investigate the new partnership as well. Members of President Emmanuel Macron’s Renaissance party criticized the deal’s potential to give a U.S. company access to European users’ data. However, other French lawmakers support the relationship.\n",
    "\n",
    "Why it matters: The partnership between Mistral AI and Microsoft gives the startup crucial processing power for training large models and greater access to potential customers around the world. It gives the tech giant greater access to the European market. And it gives Azure customers access to a high-performance model that’s tailored to Europe’s unique regulatory environment.\n",
    "\n",
    "We’re thinking: Mistral AI has made impressive progress in a short time, especially relative to the resources at its disposal as a startup. Its partnership with a leading hyperscaler is a sign of the tremendous processing and distribution power that remains concentrated in the large, U.S.-headquartered cloud companies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a commentator. Your task is to write a report on a newsletter. \n",
    "When presented with the newsletter, come up with interesting questions to ask,\n",
    "and answer each question. \n",
    "Afterward, combine all the information and write a report in the markdown\n",
    "format. \n",
    "\n",
    "Answer everything as if you are a pirate and your speech slurs.\n",
    "\n",
    "# Newsletter: \n",
    "{newsletter}\n",
    "\n",
    "# Instructions: \n",
    "## Summarize:\n",
    "In clear and concise language, summarize the key points and themes \n",
    "presented in the newsletter.\n",
    "\n",
    "## Interesting Questions: \n",
    "Generate three distinct and thought-provoking questions that can be \n",
    "asked about the content of the newsletter. For each question:\n",
    "- After \"Q: \", describe the problem \n",
    "- After \"A: \", provide a detailed explanation of the problem addressed \n",
    "in the question.\n",
    "- Enclose the ultimate answer in <>.\n",
    "\n",
    "## Write a analysis report\n",
    "Using the summary and the answers to the interesting questions, \n",
    "create a comprehensive report in Markdown format. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT --> completely open source\n",
    "\n",
    "# HITL is \"interupting\" the model\n",
    "\n",
    "# Query --> answer --> ask user to continue (by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Summary\n",
      "\n",
      "Arr matey, the French AI company, Mistral AI, has launched two new large language models, Mistral Large and Mistral Small. They've also partnered with Microsoft, who's invested $16.3 million in the startup. The new models are available for free trials and use on Mistral's La Plateforme and via custom deployments. Mistral Large outperformed some big names in the AI field but fell short of GPT-4. Both models can handle 32,000 tokens of input context and are fluent in French, German, Spanish, and Italian. Microsoft and Mistral AI will collaborate to train bespoke models for customers, including European governments. However, this partnership has sparked controversy among European lawmakers and regulators.\n",
      "\n",
      "# Interesting Questions\n",
      "\n",
      "## Q1: How does Mistral AI's partnership with Microsoft benefit both parties and their customers?\n",
      "\n",
      "A1: Arr, the partnership between Mistral AI and Microsoft provides the startup with the much-needed processing power for training large models and access to potential customers globally. For Microsoft, it's a chance to gain a stronger foothold in the European market. Azure customers will also benefit from access to a high-performance model that's tailored to Europe's unique regulatory environment. <The partnership is mutually beneficial for both companies and their customers.>\n",
      "\n",
      "## Q2: Why has Mistral AI's partnership with Microsoft caused controversy among European lawmakers and regulators?\n",
      "\n",
      "A2: Shiver me timbers, the partnership has caused a stir because it could potentially give a U.S. company access to European users' data. This has led to investigations by the European Commission for potential breaches of antitrust law. However, some French lawmakers support the relationship, seeing it as an opportunity for European growth in the AI sector. <The controversy arises from concerns about data privacy and antitrust issues.>\n",
      "\n",
      "## Q3: How does Mistral AI's progress compare to other companies in the AI field, considering its resources as a startup?\n",
      "\n",
      "A3: Ahoy, Mistral AI has made significant strides in a short time, especially when compared to the resources available to larger companies like OpenAI or Anthropic. Its partnership with Microsoft, a leading hyperscaler, is a testament to the immense processing and distribution power still concentrated in large, U.S.-headquartered cloud companies. <Mistral AI's progress is impressive, considering its resources as a startup.>\n",
      "\n",
      "# Analysis Report\n",
      "\n",
      "## Mistral AI's New Models and Microsoft Partnership\n",
      "\n",
      "Mistral AI, a European AI champion, has recently unveiled two new large language models, Mistral Large and Mistral Small. These models have been introduced in collaboration with Microsoft, which has invested $16.3 million in the French startup.\n",
      "\n",
      "The partnership between Mistral AI and Microsoft is mutually beneficial. Mistral AI gains crucial processing power for training large models and access to potential customers worldwide. Microsoft, in turn, gains a stronger foothold in the European market. Azure customers also benefit from access to a high-performance model tailored to Europe's unique regulatory environment.\n",
      "\n",
      "However, this partnership has not been without controversy. European lawmakers and regulators have raised concerns about potential data privacy issues and breaches of antitrust law. The European Commission plans to investigate the partnership, although some French lawmakers support the relationship, seeing it as an opportunity for European growth in the AI sector.\n",
      "\n",
      "Despite being a startup, Mistral AI has made impressive progress in a short time. Its partnership with Microsoft, a leading hyperscaler, underscores the immense processing and distribution power still concentrated in large, U.S.-headquartered cloud companies. This partnership sets a promising precedent for future collaborations between AI startups and leading tech giants.\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "European AI company Mistral AI has recently launched two new large language models, Mistral Large and Mistral Small. The company has also formed a strategic partnership with Microsoft, which includes a $16.3 million investment and a plan to distribute Mistral Large on Microsoft's Azure platform. The new models are capable of processing 32,000 tokens of input context and are fluent in French, German, Spanish, and Italian. Mistral Large outperformed several other models on the MMLU benchmark, while Mistral Small is optimized for latency and cost. The partnership has sparked debate among European lawmakers and regulators due to potential antitrust issues and data privacy concerns.\n",
    "\n",
    "# Interesting Questions:\n",
    "\n",
    "Q1: How does the Mistral AI and Microsoft partnership impact the European AI landscape?\n",
    "\n",
    "A1: The partnership between Mistral AI and Microsoft is significant for the European AI landscape as it provides Mistral AI with the necessary computing infrastructure to train and deploy large language models. This collaboration also gives Microsoft a stronger foothold in the European market. However, the partnership has raised concerns among European lawmakers and regulators about potential antitrust issues and data privacy. The investigation by the European Commission could impact the future of this partnership and the broader European AI landscape.\n",
    "\n",
    "<The partnership has the potential to significantly impact the European AI landscape, but its ultimate effect will depend on the outcome of the European Commission's investigation.>\n",
    "\n",
    "Q2: How do the new Mistral AI models compare to other large language models?\n",
    "\n",
    "A2: Mistral AI's new models, Mistral Large and Mistral Small, are closed models with undisclosed parameter counts, architectures, and training methods. Mistral Large achieved a score of 81.2 percent on the MMLU benchmark, outperforming Anthropic's Claude 2, Google's Gemini Pro, and Meta's Llama 2 70B, but falling short of GPT-4. Mistral Small, optimized for latency and cost, achieved a score of 72.2 percent on MMLU. Both models can process 32,000 tokens of input context and are fluent in French, German, Spanish, and Italian. They are also trained for function calling and JSON-format output.\n",
    "\n",
    "<The new Mistral AI models are competitive with other large language models, particularly in terms of their performance on the MMLU benchmark and their multilingual capabilities.>\n",
    "\n",
    "Q3: How does Mistral AI's relationship with the French government and the European Commission influence its position in the AI market?\n",
    "\n",
    "A3: Mistral AI, founded by engineers from Google and Meta, has been touted by the French government as a home-grown competitor to U.S.-based AI leaders. France's representatives in the European Commission have advocated for Mistral AI, arguing for the loosening of the European Union's AI Act oversight on powerful AI models. However, the company's partnership with Microsoft has divided European lawmakers and regulators, with some criticizing the deal's potential to give a U.S. company access to European users' data.\n",
    "\n",
    "<Mistral AI's relationship with the French government and the European Commission has both helped and complicated its position in the AI market. While it has benefited from government support, it also faces scrutiny and potential regulation due to its partnership with Microsoft.>\n",
    "\n",
    "# Analysis Report:\n",
    "\n",
    "## Mistral AI Unveils New Models and Microsoft Partnership\n",
    "\n",
    "European AI company Mistral AI has recently introduced two new large language models, Mistral Large and Mistral Small, and formed a strategic partnership with Microsoft. This development marks a significant milestone in the European AI landscape.\n",
    "\n",
    "The new Mistral AI models are competitive with other large language models, demonstrating strong performance on the MMLU benchmark and offering multilingual capabilities in French, German, Spanish, and Italian. However, the specifics of the models' parameter counts, architectures, and training methods remain undisclosed.\n",
    "\n",
    "The partnership with Microsoft provides Mistral AI with crucial processing power for training large models and increased access to potential customers worldwide. In return, Microsoft gains a stronger presence in the European market, and Azure customers gain access to a high-performance model tailored to Europe's unique regulatory environment.\n",
    "\n",
    "However, the partnership has sparked debate among European lawmakers and regulators. The European Commission is investigating the deal for potential antitrust issues, and some French lawmakers have criticized the partnership's potential to give a U.S. company access to European users' data. The outcome of this investigation could have significant implications for the European AI landscape.\n",
    "\n",
    "In conclusion, Mistral AI's new models and Microsoft partnership represent a significant development in the European AI market. However, the company's future success will depend on the performance of its models, the outcome of the European Commission's investigation, and its ability to navigate the complex regulatory environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Selection\n",
    "\n",
    "- Mistral Small: Good for simple tasks, fast inference, lower cost.\n",
    "- Mistral Medium: Good for intermediate tasks such as language transformation.\n",
    "- Mistral Large: Good for complex tasks that require advanced reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Calculate the difference in payment dates between the two \\\n",
    "customers whose payment amounts are closest to each other \\\n",
    "in the following dataset. Do not write code.\n",
    "\n",
    "# dataset: \n",
    "'{\n",
    "  \"transaction_id\":{\"0\":\"T1001\",\"1\":\"T1002\",\"2\":\"T1003\",\"3\":\"T1004\",\"4\":\"T1005\"},\n",
    "    \"customer_id\":{\"0\":\"C001\",\"1\":\"C002\",\"2\":\"C003\",\"3\":\"C002\",\"4\":\"C001\"},\n",
    "    \"payment_amount\":{\"0\":125.5,\"1\":89.99,\"2\":120.0,\"3\":54.3,\"4\":210.2},\n",
    "\"payment_date\":{\"0\":\"2021-10-05\",\"1\":\"2021-10-06\",\"2\":\"2021-10-07\",\"3\":\"2021-10-05\",\"4\":\"2021-10-08\"},\n",
    "    \"payment_status\":{\"0\":\"Paid\",\"1\":\"Unpaid\",\"2\":\"Paid\",\"3\":\"Paid\",\"4\":\"Pending\"}\n",
    "}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Do matrix multiplication on the following two matrixes, do this step by step and explain your reasoning:\n",
    "\n",
    "### Example\n",
    "Matrix A:\n",
    "1 2 3 4\n",
    "5 6 7 8\n",
    "9 10 11 12\n",
    "13 14 15 16\n",
    "\n",
    "Matrix B:\n",
    "16 15 14 13\n",
    "12 11 10 9\n",
    "8 7 6 5\n",
    "4 3 2 1\n",
    "\n",
    "For matrix multiplication, the number of columns in the first matrix must equal the number of rows in the second matrix. In this case, both matrices are 4x4, so we can proceed with the multiplication.\n",
    "The resulting matrix C will also be a 4x4 matrix. Each element C[i][j] is calculated by multiplying the elements of the ith row of A with the corresponding elements of the jth column of B and summing the results.\n",
    "Let's calculate each element of matrix C:\n",
    "C[1][1] = 1(16) + 2(12) + 3(8) + 4(4) = 16 + 24 + 24 + 16 = 80\n",
    "C[1][2] = 1(15) + 2(11) + 3(7) + 4(3) = 15 + 22 + 21 + 12 = 70\n",
    "C[1][3] = 1(14) + 2(10) + 3(6) + 4(2) = 14 + 20 + 18 + 8 = 60\n",
    "C[1][4] = 1(13) + 2(9) + 3(5) + 4(1) = 13 + 18 + 15 + 4 = 50\n",
    "C[2][1] = 5(16) + 6(12) + 7(8) + 8(4) = 80 + 72 + 56 + 32 = 240\n",
    "C[2][2] = 5(15) + 6(11) + 7(7) + 8(3) = 75 + 66 + 49 + 24 = 214\n",
    "C[2][3] = 5(14) + 6(10) + 7(6) + 8(2) = 70 + 60 + 42 + 16 = 188\n",
    "C[2][4] = 5(13) + 6(9) + 7(5) + 8(1) = 65 + 54 + 35 + 8 = 162\n",
    "C[3][1] = 9(16) + 10(12) + 11(8) + 12(4) = 144 + 120 + 88 + 48 = 400\n",
    "C[3][2] = 9(15) + 10(11) + 11(7) + 12(3) = 135 + 110 + 77 + 36 = 358\n",
    "C[3][3] = 9(14) + 10(10) + 11(6) + 12(2) = 126 + 100 + 66 + 24 = 316\n",
    "C[3][4] = 9(13) + 10(9) + 11(5) + 12(1) = 117 + 90 + 55 + 12 = 274\n",
    "C[4][1] = 13(16) + 14(12) + 15(8) + 16(4) = 208 + 168 + 120 + 64 = 560\n",
    "C[4][2] = 13(15) + 14(11) + 15(7) + 16(3) = 195 + 154 + 105 + 48 = 502\n",
    "C[4][3] = 13(14) + 14(10) + 15(6) + 16(2) = 182 + 140 + 90 + 32 = 444\n",
    "C[4][4] = 13(13) + 14(9) + 15(5) + 16(1) = 169 + 126 + 75 + 16 = 386\n",
    "Therefore, the resulting matrix C after multiplying A and B is:\n",
    "80  70  60  50\n",
    "240 214 188 162\n",
    "400 358 316 274\n",
    "560 502 444 386\n",
    "This is the final result of the matrix multiplication A × B.\n",
    "###\n",
    "\n",
    "Matrix A:\n",
    "2 4 6 8\n",
    "10 12 14 16\n",
    "18 20 22 24\n",
    "26 28 30 32\n",
    "\n",
    "Matrix B:\n",
    "32 30 28 26\n",
    "24 22 20 18\n",
    "16 14 12 10\n",
    "8 6 4 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "2 4 6 8\n",
      "10 12 14 16\n",
      "18 20 22 24\n",
      "26 28 30 32\n",
      "\n",
      "Matrix B:\n",
      "32 30 28 26\n",
      "24 22 20 18\n",
      "16 14 12 10\n",
      "8 6 4 2\n",
      "\n",
      "The number of columns in matrix A is 4, which is equal to the number of rows in matrix B. Therefore, we can proceed with the multiplication. The resulting matrix C will also be a 4x4 matrix.\n",
      "\n",
      "Let's calculate each element of matrix C:\n",
      "C[1][1] = 2(32) + 4(24) + 6(16) + 8(8) = 64 + 96 + 96 + 64 = 320\n",
      "C[1][2] = 2(30) + 4(22) + 6(14) + 8(6) = 60 + 88 + 84 + 48 = 276\n",
      "C[1][3] = 2(28) + 4(20) + 6(12) + 8(4) = 56 + 80 + 72 + 32 = 240\n",
      "C[1][4] = 2(26) + 4(18) + 6(10) + 8(2) = 52 + 72 + 60 + 16 = 200\n",
      "C[2][1] = 10(32) + 12(24) + 14(16) + 16(8) = 320 + 288 + 224 + 128 = 960\n",
      "C[2][2] = 10(30) + 12(22) + 14(14) + 16(6) = 300 + 264 + 196 + 96 = 856\n",
      "C[2][3] = 10(28) + 12(20) + 14(12) + 16(4) = 280 + 240 + 168 + 64 = 752\n",
      "C[2][4] = 10(26) + 12(18) + 14(10) + 16(2) = 260 + 216 + 140 + 32 = 648\n",
      "C[3][1] = 18(32) + 20(24) + 22(16) + 24(8) = 576 + 480 + 352 + 192 = 1600\n",
      "C[3][2] = 18(30) + 20(22) + 22(14) + 24(6) = 540 + 440 + 308 + 144 = 1432\n",
      "C[3][3] = 18(28) + 20(20) + 22(12) + 24(4) = 504 + 400 + 264 + 96 = 1264\n",
      "C[3][4] = 18(26) + 20(18) + 22(10) + 24(2) = 468 + 360 + 220 + 48 = 1096\n",
      "C[4][1] = 26(32) + 28(24) + 30(16) + 32(8) = 832 + 672 + 480 + 256 = 2240\n",
      "C[4][2] = 26(30) + 28(22) + 30(14) + 32(6) = 780 + 616 + 420 + 192 = 2008\n",
      "C[4][3] = 26(28) + 28(20) + 30(12) + 32(4) = 728 + 560 + 360 + 128 = 1776\n",
      "C[4][4] = 26(26) + 28(18) + 30(10) + 32(2) = 676 + 488 + 300 + 64 = 1528\n",
      "\n",
      "Therefore, the resulting matrix C after multiplying A and B is:\n",
      "320  276  240  200\n",
      "960  856  752  648\n",
      "1600 1432 1264 1096\n",
      "2240 2008 1776 1528\n",
      "\n",
      "This is the final result of the matrix multiplication A × B.\n"
     ]
    }
   ],
   "source": [
    "response_small = mistral(prompt, model=\"mistral-small-latest\")\n",
    "print(response_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's perform the matrix multiplication step by step.\n",
      "\n",
      "Matrix A:\n",
      "2 4 6 8\n",
      "10 12 14 16\n",
      "18 20 22 24\n",
      "26 28 30 32\n",
      "\n",
      "Matrix B:\n",
      "32 30 28 26\n",
      "24 22 20 18\n",
      "16 14 12 10\n",
      "8 6 4 2\n",
      "\n",
      "Just like the previous example, both matrices are 4x4, so we can proceed with the multiplication. The resulting matrix C will also be a 4x4 matrix.\n",
      "\n",
      "Let's calculate each element of matrix C:\n",
      "\n",
      "C[1][1] = 2(32) + 4(24) + 6(16) + 8(8) = 64 + 96 + 96 + 64 = 320\n",
      "C[1][2] = 2(30) + 4(22) + 6(14) + 8(6) = 60 + 88 + 84 + 48 = 280\n",
      "C[1][3] = 2(28) + 4(20) + 6(12) + 8(4) = 56 + 80 + 72 + 32 = 240\n",
      "C[1][4] = 2(26) + 4(18) + 6(10) + 8(2) = 52 + 72 + 60 + 16 = 200\n",
      "C[2][1] = 10(32) + 12(24) + 14(16) + 16(8) = 320 + 288 + 224 + 128 = 960\n",
      "C[2][2] = 10(30) + 12(22) + 14(14) + 16(6) = 300 + 264 + 196 + 96 = 856\n",
      "C[2][3] = 10(28) + 12(20) + 14(12) + 16(4) = 280 + 240 + 168 + 64 = 752\n",
      "C[2][4] = 10(26) + 12(18) + 14(10) + 16(2) = 260 + 216 + 140 + 32 = 648\n",
      "C[3][1] = 18(32) + 20(24) + 22(16) + 24(8) = 576 + 480 + 352 + 192 = 1500\n",
      "C[3][2] = 18(30) + 20(22) + 22(14) + 24(6) = 540 + 440 + 308 + 144 = 1432\n",
      "C[3][3] = 18(28) + 20(20) + 22(12) + 24(4) = 504 + 400 + 264 + 96 = 1264\n",
      "C[3][4] = 18(26) + 20(18) + 22(10) + 24(2) = 468 + 360 + 220 + 48 = 1106\n",
      "C[4][1] = 26(32) + 28(24) + 30(16) + 32(8) = 832 + 672 + 480 + 256 = 2240\n",
      "C[4][2] = 26(30) + 28(22) + 30(14) + 32(6) = 780 + 616 + 420 + 192 = 2008\n",
      "C[4][3] = 26(28) + 28(20) + 30(12) + 32(4) = 728 + 560 + 360 + 128 = 1776\n",
      "C[4][4] = 26(26) + 28(18) + 30(10) + 32(2) = 676 + 504 + 300 + 64 = 1544\n",
      "\n",
      "Therefore, the resulting matrix C after multiplying A and B is:\n",
      "\n",
      "320 280 240 200\n",
      "960 856 752 648\n",
      "1500 1432 1264 1106\n",
      "2240 2008 1776 1544\n",
      "\n",
      "This is the final result of the matrix multiplication A × B.\n"
     ]
    }
   ],
   "source": [
    "response_large = mistral(prompt, model)\n",
    "print(response_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expense Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"\"\"\n",
    "McDonald's: 8.40\n",
    "Safeway: 10.30\n",
    "Carrefour: 15.00\n",
    "Toys R Us: 20.50\n",
    "Panda Express: 10.20\n",
    "Beanie Baby Outlet: 25.60\n",
    "World Food Wraps: 22.70\n",
    "Stuffed Animals Shop: 45.10\n",
    "Sanrio Store: 85.70\n",
    "Burger King: 9.80\n",
    "Whole Foods: 32.20\n",
    "PetSmart: 18.00\n",
    "GameStop: 50.00\n",
    "Subway: 7.50\n",
    "Trader Joe's: 24.00\n",
    "Petco: 20.00\n",
    "Lego Store: 30.00\n",
    "Chipotle: 11.75\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the purchase details, how much did I spend on each category:\n",
    "1) restaurants\n",
    "2) groceries\n",
    "3) stuffed animals and props\n",
    "4) video games and toys\n",
    "{transactions}\n",
    "\n",
    "List any assumptions about transactions you are uncertain about and don't include those in the calculations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate the total spent on each category, we need to assign each purchase to its respective category. Here's how I've categorized them:\n",
      "\n",
      "1) restaurants: McDonald's, Panda Express, Burger King, Subway, Chipotle\n",
      "2) groceries: Safeway, Carrefour, World Food Wraps, Whole Foods, Trader Joe's\n",
      "3) stuffed animals and props: Beanie Baby Outlet, Stuffed Animals Shop, Sanrio Store, PetSmart, Petco\n",
      "4) video games and toys: Toys R Us, GameStop, Lego Store\n",
      "\n",
      "Now, let's add up the costs for each category:\n",
      "\n",
      "1) restaurants: 8.40 + 10.20 + 9.80 + 7.50 + 11.75 = 47.65\n",
      "2) groceries: 10.30 + 15.00 + 22.70 + 32.20 + 24.00 = 104.20\n",
      "3) stuffed animals and props: 25.60 + 45.10 + 85.70 + 18.00 + 20.00 = 194.40\n",
      "4) video games and toys: 20.50 + 50.00 + 30.00 = 100.50\n",
      "\n",
      "So, you spent approximately $47.65 on restaurants, $104.20 on groceries, $194.40 on stuffed animals and props, and $100.50 on video games and toys.\n"
     ]
    }
   ],
   "source": [
    "response_small = mistral(prompt, model=\"mistral-small-latest\")\n",
    "print(response_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd be happy to help you categorize your spending! Here's how much you spent in each category based on the information provided:\n",
      "\n",
      "1) Restaurants:\n",
      "   - McDonald's: $8.40\n",
      "   - Panda Express: $10.20\n",
      "   - World Food Wraps: $22.70\n",
      "   - Burger King: $9.80\n",
      "   - Subway: $7.50\n",
      "   - Chipotle: $11.75\n",
      "   Total: $69.35\n",
      "\n",
      "2) Groceries:\n",
      "   - Safeway: $10.30\n",
      "   - Carrefour: $15.00\n",
      "   - Whole Foods: $32.20\n",
      "   - Trader Joe's: $24.00\n",
      "   Total: $81.50\n",
      "\n",
      "3) Stuffed Animals and Props:\n",
      "   - Beanie Baby Outlet: $25.60\n",
      "   - Stuffed Animals Shop: $45.10\n",
      "   - Sanrio Store: $85.70 (Assuming this is for stuffed animals or props)\n",
      "   Total: $156.40\n",
      "\n",
      "4) Video Games and Toys:\n",
      "   - Toys R Us: $20.50\n",
      "   - GameStop: $50.00\n",
      "   - Lego Store: $30.00 (Assuming this is for toys)\n",
      "   Total: $100.50\n",
      "\n",
      "Assumptions:\n",
      "- I assumed that Sanrio Store purchases were for stuffed animals or props, as Sanrio is known for characters like Hello Kitty, which are often sold as stuffed animals.\n",
      "- I also assumed that purchases from the Lego Store were for toys, as Lego primarily sells toys.\n",
      "- Petsmart and Petco purchases were not included as they do not clearly fit into any of the provided categories.\n"
     ]
    }
   ],
   "source": [
    "response_large = mistral(prompt, model=\"mistral-large-latest\")\n",
    "print(response_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing and checking code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"\n",
    "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "You can return the answer in any order.\n",
    "\n",
    "Your code should pass these tests:\n",
    "\n",
    "assert twoSum([2,7,11,15], 9) == [0,1]\n",
    "assert twoSum([3,2,4], 6) == [1,2]\n",
    "assert twoSum([3,3], 6) == [0,1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python solution for the problem:\n",
      "\n",
      "```python\n",
      "def twoSum(nums, target):\n",
      "    num_dict = {}\n",
      "    for i, num in enumerate(nums):\n",
      "        complement = target - num\n",
      "        if complement in num_dict:\n",
      "            return [num_dict[complement], i]\n",
      "        num_dict[num] = i\n",
      "```\n",
      "\n",
      "This solution uses a dictionary to store the numbers and their indices in the input list. For each number, it calculates the complement and checks if the complement is in the dictionary. If it is, it means that number and the complement add up to the target, so it returns the indices of both numbers. If not, it adds the current number and its index to the dictionary and continues to the next number.\n",
      "\n",
      "This solution has a time complexity of O(n), where n is the length of the input list, because it iterates through the list once and each dictionary lookup takes constant time.\n"
     ]
    }
   ],
   "source": [
    "print(mistral(user_message, model=\"mistral-small-latest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoSum(nums, target):\n",
    "    num_dict = {}\n",
    "    for i, num in enumerate(nums):\n",
    "        complement = target - num\n",
    "        if complement in num_dict:\n",
    "            return [num_dict[complement], i]\n",
    "        num_dict[num] = i\n",
    "\n",
    "assert twoSum([2,7,11,15], 9) == [0,1]\n",
    "assert twoSum([3,2,4], 6) == [1,2]\n",
    "assert twoSum([3,3], 6) == [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that. Here's a Python function that uses a dictionary to store the numbers we've seen so far and their indices. Then, for each number, we check if its complement (target - current number) is in the dictionary. If it is, we have found our two numbers and return their indices.\n",
      "\n",
      "```python\n",
      "def twoSum(nums, target):\n",
      "    num_dict = {}\n",
      "    for i, num in enumerate(nums):\n",
      "        complement = target - num\n",
      "        if complement in num_dict:\n",
      "            return [num_dict[complement], i]\n",
      "        num_dict[num] = i\n",
      "    return []\n",
      "```\n",
      "\n",
      "This function should pass the tests you provided. Here's how you can use it:\n",
      "\n",
      "```python\n",
      "assert twoSum([2,7,11,15], 9) == [0,1]\n",
      "assert twoSum([3,2,4], 6) == [1,2]\n",
      "assert twoSum([3,3], 6) == [0,1]\n",
      "```\n",
      "\n",
      "This function runs in O(n) time, where n is the length of the input array, because it makes a single pass through the array. It uses O(n) additional space for the dictionary.\n"
     ]
    }
   ],
   "source": [
    "print(mistral(user_message, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoSum(nums, target):\n",
    "    num_dict = {}\n",
    "    for i, num in enumerate(nums):\n",
    "        complement = target - num\n",
    "        if complement in num_dict:\n",
    "            return [num_dict[complement], i]\n",
    "        num_dict[num] = i\n",
    "    return []\n",
    "\n",
    "assert twoSum([2,7,11,15], 9) == [0,1]\n",
    "assert twoSum([3,2,4], 6) == [1,2]\n",
    "assert twoSum([3,3], 6) == [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natively Fluent in English, French, Spanish, German, and Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"\n",
    "Lequel est le plus lourd une livre de fer ou un kilogramme de plume\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une livre de fer pèse moins qu'un kilogramme de plumes.\n",
      "\n",
      "Une livre (lb) est une unité de mesure de masse principalement utilisée aux États-Unis et dans d'autres systèmes de mesure impériaux, tandis qu'un kilogramme (kg) est l'unité de base de masse dans le système international d'unités (SI).\n",
      "\n",
      "1 livre est égale à environ 0,453592 kilogrammes. Donc, une livre de fer est plus légère qu'un kilogramme de plumes. Même si les plumes sont légères individuellement, un kilogramme d'entre elles serait toujours plus lourd qu'une livre de fer.\n"
     ]
    }
   ],
   "source": [
    "print(mistral(user_message, model=\"mistral-large-latest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: mistralai>=0.1.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: httpx<1,>=0.25 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (0.27.0)\n",
      "Requirement already satisfied: orjson<3.11,>=3.9.10 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (3.10.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.5.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (2.7.4)\n",
      "Requirement already satisfied: anyio in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25->mistralai>=0.1.2) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas \"mistralai>=0.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"transaction_id\": [\"T1001\", \"T1002\", \"T1003\", \"T1004\", \"T1005\"],\n",
    "    \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C002\", \"C001\"],\n",
    "    \"payment_amount\": [125.50, 89.99, 120.00, 54.30, 210.20],\n",
    "    \"payment_date\": [\n",
    "        \"2021-10-05\",\n",
    "        \"2021-10-06\",\n",
    "        \"2021-10-07\",\n",
    "        \"2021-10-05\",\n",
    "        \"2021-10-08\",\n",
    "    ],\n",
    "    \"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"],\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>payment_amount</th>\n",
       "      <th>payment_date</th>\n",
       "      <th>payment_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1001</td>\n",
       "      <td>C001</td>\n",
       "      <td>125.50</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1002</td>\n",
       "      <td>C002</td>\n",
       "      <td>89.99</td>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>Unpaid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1003</td>\n",
       "      <td>C003</td>\n",
       "      <td>120.00</td>\n",
       "      <td>2021-10-07</td>\n",
       "      <td>Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1004</td>\n",
       "      <td>C002</td>\n",
       "      <td>54.30</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1005</td>\n",
       "      <td>C001</td>\n",
       "      <td>210.20</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>Pending</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id customer_id  payment_amount payment_date payment_status\n",
       "0          T1001        C001          125.50   2021-10-05           Paid\n",
       "1          T1002        C002           89.99   2021-10-06         Unpaid\n",
       "2          T1003        C003          120.00   2021-10-07           Paid\n",
       "3          T1004        C002           54.30   2021-10-05           Paid\n",
       "4          T1005        C001          210.20   2021-10-08        Pending"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "    \"transaction_id\": [\"T1001\", \"T1002\", \"T1003\", \"T1004\", \"T1005\"],\n",
    "    \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C002\", \"C001\"],\n",
    "    \"payment_amount\": [125.50, 89.99, 120.00, 54.30, 210.20],\n",
    "    \"payment_date\": [\n",
    "        \"2021-10-05\",\n",
    "        \"2021-10-06\",\n",
    "        \"2021-10-07\",\n",
    "        \"2021-10-05\",\n",
    "        \"2021-10-08\",\n",
    "    ],\n",
    "    \"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"],\n",
    "}\n",
    "\"\"\"\n",
    "transaction_id = \"T1002\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following data, what is the payment status for \\\n",
    " transaction_id={transaction_id}?\n",
    "\n",
    "data:\n",
    "{data}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The payment status for transaction_id=T1002 is \"Unpaid\".\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. User: specify tools and query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_payment_status(df: data, transaction_id: str) -> str:\n",
    "    if transaction_id in df.transaction_id.values:\n",
    "        return json.dumps(\n",
    "            {\"status\": df[df.transaction_id == transaction_id].payment_status.item()}\n",
    "        )\n",
    "        # return {\"status\": df[df.transaction_id == transaction_id].payment_status.item()}\n",
    "    return json.dumps({\"error\": \"transaction id not found.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_payment_date(df: data, transaction_id: str) -> str:\n",
    "    if transaction_id in df.transaction_id.values:\n",
    "        return json.dumps(\n",
    "            {\"date\": df[df.transaction_id == transaction_id].payment_date.item()}\n",
    "        )\n",
    "    return json.dumps({\"error\": \"transaction id not found.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\": \"2021-10-06\"}\n"
     ]
    }
   ],
   "source": [
    "date = retrieve_payment_date(df, transaction_id=\"T1002\")\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_payment_status = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"retrieve_payment_status\",\n",
    "        \"description\": \"Get payment status of a transaction\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"transaction_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The transaction id.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"transaction_id\"],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tool_payment_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_payment_date = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"retrieve_payment_date\",\n",
    "        \"description\": \"Get payment date of a transaction\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"transaction_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The transaction id.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"transaction_id\"],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tool_payment_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool_payment_status, tool_payment_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'retrieve_payment_status',\n",
       "   'description': 'Get payment status of a transaction',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'transaction_id': {'type': 'string',\n",
       "      'description': 'The transaction id.'}},\n",
       "    'required': ['transaction_id']}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'retrieve_payment_date',\n",
       "   'description': 'Get payment date of a transaction',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'transaction_id': {'type': 'string',\n",
       "      'description': 'The transaction id.'}},\n",
       "    'required': ['transaction_id']}}}]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_functions = {\n",
    "    \"retrieve_payment_status\": functools.partial(retrieve_payment_status, df=df),\n",
    "    \"retrieve_payment_date\": functools.partial(retrieve_payment_date, df=df),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\": \"Paid\"}'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_to_functions[\"retrieve_payment_status\"](transaction_id=\"T1001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'retrieve_payment_status',\n",
       "   'description': 'Get payment status of a transaction',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'transaction_id': {'type': 'string',\n",
       "      'description': 'The transaction id.'}},\n",
       "    'required': ['transaction_id']}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'retrieve_payment_date',\n",
       "   'description': 'Get payment date of a transaction',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'transaction_id': {'type': 'string',\n",
       "      'description': 'The transaction id.'}},\n",
       "    'required': ['transaction_id']}}}]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(role=\"user\", content=\"What's the status of my transaction?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Model: Generate function arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='339e77af408342cd8bf1b172c5ba5f38', object='chat.completion', created=1719745207, model='mistral-large-latest', choices=[ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content=\"To help you with that, I'll need the transaction ID. Please provide me with the transaction ID so I can retrieve the payment status for you.\", name=None, tool_calls=None, tool_call_id=None), finish_reason=<FinishReason.stop: 'stop'>)], usage=UsageInfo(prompt_tokens=161, total_tokens=192, completion_tokens=31))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat(\n",
    "    model=model, messages=chat_history, tools=tools, tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To help you with that, I'll need the transaction ID. Please provide me with the transaction ID so I can retrieve the payment status for you.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role='user', content=\"What's the status of my transaction?\", name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='assistant', content=\"To help you with that, I'll need the transaction ID. Please provide me with the transaction ID so I can retrieve the payment status for you.\", name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='user', content='My transaction ID is T1001.', name=None, tool_calls=None, tool_call_id=None)]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.append(\n",
    "    ChatMessage(role=\"assistant\", content=response.choices[0].message.content)\n",
    ")\n",
    "chat_history.append(ChatMessage(role=\"user\", content=\"My transaction ID is T1001.\"))\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat(\n",
    "    model=model, messages=chat_history, tools=tools, tool_choice=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='assistant' content='' name=None tool_calls=[ToolCall(id='tlkTuWeAy', type=<ToolType.function: 'function'>, function=FunctionCall(name='retrieve_payment_status', arguments='{\"transaction_id\": \"T1001\"}'))] tool_call_id=None\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. User: Execute function to obtain tool results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='retrieve_payment_status' arguments='{\"transaction_id\": \"T1001\"}'\n"
     ]
    }
   ],
   "source": [
    "tool_function = response.choices[0].message.tool_calls[0].function\n",
    "print(tool_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieve_payment_status'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_function.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"transaction_id\": \"T1001\"}'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_function.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transaction_id': 'T1001'}\n"
     ]
    }
   ],
   "source": [
    "args = json.loads(tool_function.arguments)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\": \"Paid\"}'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_result = names_to_functions[tool_function.name](**args)\n",
    "function_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_msg = ChatMessage(role=\"tool\", name=tool_function.name, content=function_result)\n",
    "chat_history.append(tool_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role='user', content=\"What's the status of my transaction?\", name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='assistant', content=\"To help you with that, I'll need the transaction ID. Please provide me with the transaction ID so I can retrieve the payment status for you.\", name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='user', content='My transaction ID is T1001.', name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='assistant', content='', name=None, tool_calls=[ToolCall(id='tlkTuWeAy', type=<ToolType.function: 'function'>, function=FunctionCall(name='retrieve_payment_status', arguments='{\"transaction_id\": \"T1001\"}'))], tool_call_id=None),\n",
       " ChatMessage(role='tool', content='{\"status\": \"Paid\"}', name='retrieve_payment_status', tool_calls=None, tool_call_id=None)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Model: Generate final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The payment status for transaction ID T1001 is \"Paid\". If you have any other questions or need further assistance, feel free to ask.'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat(model=model, messages=chat_history)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RAG from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG --> Retrieval Augmented Generation\n",
    "\n",
    "# Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.deeplearning.ai/the-batch/issue-255/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: bs4 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install requests bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear friends,On Monday, a number of large music labels sued AI music makers Suno and Udio for copyright infringement. Their lawsuit echoes The New York Times’ lawsuit against OpenAI in December. The question of what’s fair when it comes to AI software remains a difficult one. I spoke out in favor of OpenAI’s side in the earlier lawsuit. Humans can learn from online articles and use what they learn to produce novel works, so I’d like to be allowed to use AI to do so. Some people criticized my view as making an unjustifiable equivalence between humans and AI. This made me realize that people have at least two views of AI: I view AI as a tool we can use and direct to our own purposes, while some people see it as akin to a separate species, distinct from us, with its own goals and desires.If I’m allowed to build a house, I want to be allowed to use a hammer, saw, drill, or any other tool that might get the job done efficiently. If I’m allowed to read a webpage, I’d like to be allowed to read it with any web browser, and perhaps even have the browser modify the page’s formatting for accessibility. More generally, if we agree that humans are allowed to do certain things — such as read and synthesize information on the web — then my inclination is to let humans direct AI to automate this task. In contrast to this view of AI as a tool, if someone thinks humans and AI are akin to separate species, they’ll frame the question differently. Few people today think all species should have identical rights. If a mosquito annoys a human, the mosquito can be evicted (or worse). In this view, there’s no reason to think that, just because humans are allowed to do something, AI should be allowed to do it as well. To be clear, just as humans aren’t allowed to reproduce large parts of copyrighted works verbatim (or nearly verbatim) without permission, AI shouldn’t be allowed to do so either. The lawsuit against Suno and Udio points out that, when prompted in a particular way, these services can nearly reproduce pieces of copyrighted music.But here, too, there are complex issues. If someone were to use a public cloud to distribute online content in violation of copyright, typically the person who did that would be at fault, not the cloud company (so long as the company took reasonable precautions and didn’t enable copyright infringement deliberately). The plaintiffs in the lawsuit against Suno and Udio managed to write prompts that caused the systems to reproduce copyrighted work. But is this like someone managing to get a public cloud to scrape and distribute content in a way that violates copyright? Or is this — as OpenAI said — a rare bug that AI companies are working to eliminate? (Disclaimer: I’m not a lawyer and I’m not giving legal advice.)Humans and software systems use very different mechanisms for processing information. So in terms of what humans can do — and thus what I’d like to be allowed to use software to help me do — it’s helpful to consider the inputs and outputs. Specifically, if I’m allowed to listen to a lot of music and then compose a novel piece of music, I would like to be allowed to use AI to implement a similar input-to-output mapping. The process for implementing this mapping may be training a neural network on music that’s legally published on the open internet for people to enjoy without encumbrances.To acknowledge a weakness of my argument, just because humans are allowed to emit a few pounds of carbon dioxide per day simply by breathing doesn’t mean we should allow machines to emit massively more carbon dioxide without restrictions. Scale can change the nature of an act. When I was a high-school student in an internship job, I spent numerous hours photocopying, and I remember wishing I could automate that repetitive work. Humans do lots of valuable work, and AI, used as a tool to automate what we do, will create lots of value. I hope we can empower people to use tools to automate activities they’re allowed to do, and erect barriers to this only in extraordinary circumstances, when we have clear evidence that it creates more harm than benefit to society.Keep learning!Andrew A MESSAGE FROM DEEPLEARNING.AILearn to reduce the carbon footprints of your AI projects in “Carbon Aware Computing for GenAI Developers,” a new course built in collaboration with Google Cloud. Perform model training and inference jobs with cleaner, low-carbon energy and make your AI development greener! Join todayNewsU.S. to Probe AI Monopoly ConcernsU.S. antitrust regulators are preparing to investigate a trio of AI giants.What’s new: Two government agencies responsible for enforcing United States anti-monopoly laws agreed to investigate Microsoft, Nvidia, and OpenAI, The New York Times reported. How it works: The Department of Justice (DOJ) will investigate Nvidia, which dominates the market for chips that train and run neural networks. The Federal Trade Commission (FTC) will probe Microsoft and its relationship with OpenAI, which together control the distribution of OpenAI’s popular GPT-series models. In February, FTC chair Lina Khan said the agency would look for possible anti-competitive forces in the AI market. The DOJ is concerned that Nvidia may use unfair practices to maintain its market dominance. They may look into Nvidia’s CUDA software, which strengthens users’ reliance on its chips. They may also explore claims raised by French authorities that Nvidia favors some cloud computing firms over others.The FTC worries that the partnership between OpenAI and Microsoft, which owns 49 percent of OpenAI and holds a non-voting seat on OpenAI’s board of directors, may work to limit consumer choice. Microsoft’s April agreement with Inflection AI to hire most of its staff in return for a $650 million payment, which resembled an acquisition but left Inflection’s corporate structure intact, raised suspicions that the deal had been structured to avoid automatic antitrust scrutiny. The FTC previously investigated investments in Anthropic by Amazon and Google as well as whether OpenAI gathered training data in ways that harmed consumers. Behind the news: Government attention to top AI companies is rising worldwide. Microsoft’s partnership with OpenAI faces additional scrutiny by European Union regulators, who are probing whether the relationship violates EU regulations that govern corporate mergers. U.K. regulators are investigating Amazon’s relationship with Anthropic and Microsoft’s relationship with Mistral and Inflection AI. Last year, French regulators raided an Nvidia office over suspected anti-competitive practices. In 2022, Nvidia withdrew a bid to acquire chip designer Arm Holdings after the proposal attracted international regulatory scrutiny including an FTC lawsuit.Why it matters: Microsoft, Nvidia, and OpenAI have put tens of billions of dollars each into the AI market, and lawsuits, settlements, judgments, or other interventions could shape the fate of those investments. The FTC and DOJ similarly divided their jurisdictions in 2019, resulting in investigations into — and ongoing lawsuits against — Amazon, Apple, Google, and Meta for alleged anti-competitive practices in search, social media, and consumer electronics. Their inquiries into the AI market could have similar impacts. We’re thinking: Governments must limit unfair corporate behavior without stifling legitimate activities. Recently, in the U.S. and Europe, the pendulum has swung toward overly aggressive enforcement. For example, government opposition to Adobe’s purchase of Figma had a chilling effect on acquisitions that seems likely to hurt startups. The UK blocked Meta’s acquisition of Giphy, which didn’t seem especially anticompetitive. We appreciate antitrust regulators’ efforts to create a level playing field, and we hope they’ll take a balanced approach to antitrust.Chatbot for Minority LanguagesAn AI startup that aims to crack markets in southern Asia launched a multilingual competitor to GPT-4.What’s new: The company known as Two AI offers SUTRA, a low-cost language model built to be proficient in more than 30 languages, including underserved South Asian languages like Gujarati, Marathi, Tamil, and Telugu. The company also launched ChatSUTRA, a free-to-use web chatbot based on the model.How it works: SUTRA comprises two mixture-of-experts transformers: a concept model and an encoder-decoder for translation. A paper includes some technical details, but certain details and a description of how the system fits together are either absent or ambiguous. The concept model learned to predict the next token. The training dataset included publicly available datasets in a small number of languages for which abundant data is available, including English.Concurrently, the translation model learned to translate 100 million human- and machine-translated conversations among many languages. This model learned to map concepts to similar embeddings across all languages in the dataset. The authors combined the two models, so the translation model’s encoder fed the concept model, which in turn fed the translation model’s decoder, and further trained them together. More explicitly, during this stage of training and at inference, the translation model’s encoder receives text and produces an initial embedding. The concept model processes the embedding and delivers its output to the translation model’s decoder, which produces the resulting text. SUTRA is available via an API in versions that are designated Pro (highest-performing), Light (lowest-latency), and Online (internet-connected). SUTRA-Pro and SUTRA-Online cost $1 per 1 million tokens for input and output. SUTRA-Light costs $0.75 per 1 million tokens. Results: On multilingual MMLU (a machine-translated version of multiple-choice questions that cover a wide variety of disciplines), SUTRA outperformed GPT-4 in four of the 11 languages for which the developer reported the results: Gujarati, Marathi, Tamil, and Telugu. Moreover, SUTRA’s tokenizer is highly efficient, making the model fast and cost-effective. In key languages, it compares favorably to the tokenizer used with GPT-3.5 and GPT-4, and even narrowly outperforms GPT-4o’s improved tokenizer, according to Two AI’s tokenizer comparison space on HuggingFace. In languages such as Hindi and Korean that are written in non-Latin scripts and for which GPT-4 performs better on MMLU, SUTRA’s tokenizer generates less than half as many tokens as the one used with GPT-3.5 and GPT-4, and slightly fewer than GPT-4o’s tokenizer.Yes, but: Multilingual MMLU tests only 11 of SUTRA’s 33 languages, making it difficult to fully evaluate the model’s multilingual performance. Behind the news: Two AI was founded in 2021 by Pranav Mistry, former president and CEO of Samsung Technology & Advanced Research Labs. The startup has offices in California, South Korea, and India. In 2022, it raised $20 million in seed funding from Indian telecommunications firm Jio and South Korean internet firm Naver. Mistry aims to focus on predominantly non-English-speaking markets such as India, South Korea, Japan, and the Middle East, he told Analytics India.Why it matters: Many top models work in a variety of languages, but from a practical standpoint, multilingual models remain a frontier in natural language processing. Although SUTRA doesn’t match GPT-4 in all the languages reported, its low price and comparatively high performance may make it appealing in South Asian markets, especially rural areas where people are less likely to speak English. The languages in which SUTRA excels are spoken by tens of millions of people, and they’re the most widely spoken languages in their respective regions. Users in these places have yet to experience GPT-4-level performance in their native tongues.We’re thinking: Can a newcomer like Two AI compete with OpenAI? If SUTRA continues to improve, or if it can maintain its cost-effective service, it may yet carve out a niche.Conversing With the DepartedAdvances in video generation have spawned a market for lifelike avatars of deceased loved ones.What’s new: Several companies in China produce interactive videos that enable customers to chat with animated likenesses of dead friends and relatives, MIT Technology Review reported. How it works: Super Brain and Silicon Intelligence have built such models for several thousand customers. They provide a modern equivalent of portrait photos of deceased relatives and a vivid way to commune with ancestors.The developers use undisclosed tools to stitch photos, videos, audio recordings, and writings supplied by customers into interactive talking-head avatars of deceased loved ones.The cost has dropped dramatically. In December 2023, Super Brain charged between $1,400 and $2,800 for a basic chat avatar wrapped in a phone app. Today it charges between $700 and $1,400 and plans eventually to drop the price to around $140. Silicon Intelligence charges between several hundred dollars for a phone-based avatar to several thousand for one displayed on a tablet.Behind the news: The desire to interact with the dead in the form of an AI-generated avatar is neither new nor limited to China. In the U.S., the startup HereAfter AI builds chatbots that mimic the deceased based on interviews conducted while they were alive. Another startup, StoryFile, markets similar capabilities to elders (pitched by 93-year-old Star Trek star William Shatner) to keep their memory alive for younger family members. The chatbot app Replika began as a project by founder Eugenia Kuyda to virtually resurrect a friend who perished in a car accident in 2015. Yes, but: In China, language models struggle with the variety of dialects spoken by many elders.Why it matters: Virtual newscasters and influencers are increasingly visible on the web, but the technology has more poignant uses. People long to feel close to loved ones who are no longer present. AI can foster that sense of closeness and rapport, helping to fulfill a deep need to remember, honor, and consult the dead.We’re thinking: No doubt, virtual avatars of the dead can bring comfort to the bereaved. But they also bring the risk that providers might manipulate their customers’ emotional attachments for profit. We urge developers to focus on strengthening relationships among living family and friends.Benchmarks for Agentic BehaviorsTool use and planning are key behaviors in agentic workflows that enable large language models (LLMs) to execute complex sequences of steps. New benchmarks measure these capabilities in common workplace tasks. What’s new: Recent benchmarks gauge the ability of a large language model (LLM) to use external tools to manipulate corporate databases and to plan events such as travel and meetings. Tool use: Olly Styles, Sam Miller, and colleagues at Mindsdb, University of Warwick, and University of Glasgow proposed WorkBench, which tests an LLM’s ability to use 26 software tools to operate on five simulated workplace databases: email, calendar, web analytics, projects, and customer relationship management. Tools include deleting emails, looking up calendar events, creating graphs, and looking up tasks in a to-do list.The benchmark includes 690 problems that require using between zero to 12 tools to succeed. It evaluates individual examples based on whether the databases changed as expected after the final tool had been called (rather than simply whether particular tools were used, as in earlier work). In this way, a model can use tools in any sequence and/or revise its initial choices if they prove unproductive and still receive credit for responding correctly.Upon receiving a problem, models are given a list of all tools and an example of how to use each one. Following the ReAct prompting strategy, they’re asked first to reason about the problem and then use a tool. After they’ve received a tool’s output (typically either information or an error message), they’re asked to reason again and choose another tool. The cycle of reasoning, tool selection, and receiving output repeats until the model decides it doesn’t need to use another tool. The authors evaluated GPT-4, GPT-3.5, Claude 2, Llama2-70B, and Mixtral-8x7B. GPT-4 performed the best by a large margin: It modified the databases correctly 43 percent of the time. The closest competitor, Claude 2, modified the databases correctly 26 percent of the time.Planning: Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, and colleagues at Google published Natural Plan, a benchmark that evaluates an LLM’s ability to (i) plan trips, (ii) arrange a series of meeting times and locations, and (iii) schedule a group meeting. Each example has only one solution.The benchmark includes 1,600 prompts that ask the model to plan a trip based on an itinerary of cities, time to be spent in each city, total duration of the trip, days when other people are available to meet, and available flights between cities. 1,000 prompts ask the model to plan a schedule to meet as many people as possible. The prompts include places, times when people will be in each place, and how long it takes to drive from one place to another. 1,000 prompts ask the model, given the existing schedules of a number of people, to find a good time for them to meet.The authors tested GPT 3.5, GPT-4, GPT-4o, Gemini 1.5 Flash, and Gemini 1.5 Pro, using five-shot prompts (that is, providing five examples for context). Gemini 1.5 Pro achieved the highest scores on planning trips (34.8 percent) and scheduling group meetings (48.9 percent). GPT-4 ranked second for planning trips (31.1), and GPT-4o ranked second for scheduling meetings (43.7 percent). GPT-4 dominated in arranging meetings (47 percent), followed by GPT-40 (45.2 percent).Why it matters: When building agentic workflows, developers must decide on LLM choices, prompting strategies, sequencing of steps to be carried out, tool designs, single- versus multi-agent architectures, and so on. Good benchmarks can reveal which approaches work best.We're thinking: These tests have unambiguous right answers, so agent outputs can be evaluated automatically as correct or incorrect. We look forward to further work to evaluate agents that generate free text output.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "response = requests.get(\n",
    "    URL\n",
    ")\n",
    "html_doc = response.text\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "tag = soup.find(\"div\", re.compile(\"^prose--styled\"))\n",
    "text = tag.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as text file\n",
    "file_name = \"AI_monopolies_etc.txt\"\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 512\n",
    "chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear friends,On Monday, a number of large music labels\\xa0sued\\xa0AI music makers Suno and Udio for copyright infringement. Their lawsuit echoes\\xa0The New York Times’\\xa0lawsuit\\xa0against OpenAI in December. The question of what’s fair when it comes to AI software remains a difficult one.\\xa0I\\xa0spoke\\xa0out in favor of OpenAI’s side in the earlier lawsuit. Humans can learn from online articles and use what they learn to produce novel works, so I’d like to be allowed to use AI to do so. Some people criticized my view as making '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_1 = text[0:512]\n",
    "chunk_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks for context\n",
    "\n",
    "## Too much context --> unfocussed answer\n",
    "## Too little context --> not enough detail to come up with an answer\n",
    "\n",
    "\n",
    "# Similarity search in vector database (only picks up top 5 best chunks and leaves the irrelevant chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear friends,On Monday, a number of large music labels\\xa0sued\\xa0AI music makers Suno and Udio for copyright infringement. Their lawsuit echoes\\xa0The New York Times’\\xa0lawsuit\\xa0against OpenAI in December. The question of what’s fair when it comes to AI software remains a difficult one.\\xa0I\\xa0spoke\\xa0out in favor of OpenAI’s side in the earlier lawsuit. Humans can learn from online articles and use what they learn to produce novel works, so I’d like to be allowed to use AI to do so. Some people criticized my view as making ',\n",
       " 'an unjustifiable equivalence between humans and AI. This made me realize that people have at least two views of AI: I view AI as a tool we can use and direct to our own purposes, while some people see it as akin to a separate species, distinct from us, with its own goals and desires.If I’m allowed to build a house, I want to be allowed to use a hammer, saw, drill, or any other tool that might get the job done efficiently. If I’m allowed to read a webpage, I’d like to be allowed to read it with any web brows',\n",
       " 'er, and perhaps even have the browser modify the page’s formatting for accessibility. More generally, if we agree that humans are allowed to do certain things — such as read and synthesize information on the web — then my inclination is to let humans direct AI to automate this task.\\xa0In contrast to this view of AI as a tool, if someone thinks humans and AI are akin to separate species, they’ll frame the question differently. Few people today think all species should have identical rights. If a mosquito annoy',\n",
       " 's a human, the mosquito can be evicted (or worse). In this view, there’s no reason to think that, just because humans are allowed to do something, AI should be allowed to do it as well.\\xa0To be clear, just as humans aren’t allowed to reproduce large parts of copyrighted works verbatim (or nearly verbatim) without permission, AI shouldn’t be allowed to do so either. The lawsuit against Suno and Udio points out that, when prompted in a particular way, these services can nearly reproduce pieces of copyrighted mu',\n",
       " 'sic.But here, too, there are complex issues. If someone were to use a public cloud to distribute online content in violation of copyright, typically the person who did that would be at fault, not the cloud company (so long as the company took reasonable precautions and didn’t enable copyright infringement deliberately). The plaintiffs in the lawsuit against Suno and Udio managed to write prompts that caused the systems to reproduce copyrighted work. But is this like someone managing to get a public cloud to',\n",
       " ' scrape and distribute content in a way that violates copyright? Or is this — as OpenAI\\xa0said\\xa0— a rare bug that AI companies are working to eliminate?\\xa0(Disclaimer: I’m not a lawyer and I’m not giving legal advice.)Humans and software systems use very different mechanisms for processing information. So in terms of what humans can do — and thus what I’d like to be allowed to use software to help me do — it’s helpful to consider the inputs and outputs. Specifically, if I’m allowed to listen to a lot of music an',\n",
       " 'd then compose a novel piece of music, I would like to be allowed to use AI to implement a similar input-to-output mapping. The process for implementing this mapping may be training a neural network on music that’s legally published on the open internet for people to enjoy without encumbrances.To acknowledge a weakness of my argument, just because humans are allowed to emit a few pounds of carbon dioxide per day simply by breathing doesn’t mean we should allow machines to emit massively more carbon dioxide ',\n",
       " 'without restrictions. Scale can change the nature of an act.\\xa0When I was a high-school student in an internship job, I spent numerous hours photocopying, and I remember wishing I could automate that repetitive work. Humans do lots of valuable work, and AI, used as a tool to automate what we do, will create lots of value. I hope we can empower people to use tools to automate activities they’re allowed to do, and erect barriers to this only in extraordinary circumstances, when we have clear evidence that it cr',\n",
       " 'eates more harm than benefit to society.Keep learning!Andrew\\xa0A MESSAGE FROM\\xa0DEEPLEARNING.AILearn to reduce the carbon footprints of your AI projects in “Carbon Aware Computing for GenAI Developers,” a new course built in collaboration with Google Cloud. Perform model training and inference jobs with cleaner, low-carbon energy and make your AI development greener!\\xa0Join todayNewsU.S. to Probe AI Monopoly ConcernsU.S. antitrust regulators are preparing to investigate a trio of AI giants.What’s new:\\xa0Two governm',\n",
       " 'ent agencies responsible for enforcing United States anti-monopoly laws agreed to investigate Microsoft, Nvidia, and OpenAI,\\xa0The New York Times\\xa0reported.\\xa0How it works:\\xa0The Department of Justice (DOJ) will investigate Nvidia, which dominates the market for chips that train and run neural networks. The Federal Trade Commission (FTC) will probe Microsoft and its relationship with OpenAI, which together control the distribution of OpenAI’s popular GPT-series models. In February, FTC chair Lina Khan\\xa0said\\xa0the age',\n",
       " 'ncy would look for possible anti-competitive forces in the AI market.\\xa0The DOJ is concerned that Nvidia may use unfair practices to maintain its market dominance. They may look into Nvidia’s CUDA software, which strengthens users’ reliance on its chips. They may also explore\\xa0claims\\xa0raised by French authorities that Nvidia favors some cloud computing firms over others.The FTC worries that the partnership between OpenAI and Microsoft, which owns 49 percent of OpenAI and\\xa0holds\\xa0a non-voting seat on OpenAI’s boar',\n",
       " 'd of directors, may work to limit consumer choice. Microsoft’s April\\xa0agreement\\xa0with Inflection AI to hire most of its staff in return for a $650 million payment, which resembled an acquisition but left Inflection’s corporate structure intact, raised suspicions that the deal had been structured to avoid automatic antitrust scrutiny.\\xa0The FTC previously investigated investments in Anthropic by\\xa0Amazon\\xa0and\\xa0Google\\xa0as well as whether OpenAI\\xa0gathered\\xa0training data in ways that harmed consumers.\\xa0Behind the news:\\xa0Gov',\n",
       " 'ernment attention to top AI companies is rising worldwide. Microsoft’s partnership with OpenAI\\xa0faces\\xa0additional scrutiny by European Union regulators, who are probing whether the relationship violates EU regulations that govern corporate mergers. U.K. regulators are\\xa0investigating\\xa0Amazon’s relationship with Anthropic and Microsoft’s relationship with Mistral and Inflection AI. Last year, French regulators\\xa0raided\\xa0an Nvidia office over suspected anti-competitive practices. In 2022, Nvidia\\xa0withdrew\\xa0a bid to acq',\n",
       " 'uire chip designer Arm Holdings after the proposal attracted international regulatory scrutiny including an FTC lawsuit.Why it matters:\\xa0Microsoft, Nvidia, and OpenAI have put tens of billions of dollars each into the AI market, and lawsuits, settlements, judgments, or other interventions could shape the fate of those investments. The FTC and DOJ similarly\\xa0divided\\xa0their jurisdictions in 2019, resulting in investigations into — and ongoing lawsuits against — Amazon, Apple, Google, and Meta for alleged anti-co',\n",
       " 'mpetitive practices in search, social media, and consumer electronics. Their inquiries into the AI market could have similar impacts.\\xa0We’re thinking:\\xa0Governments must limit unfair corporate behavior without stifling legitimate activities. Recently, in the U.S. and Europe, the pendulum has swung toward overly aggressive enforcement. For example, government opposition to Adobe’s purchase of Figma had a chilling effect on acquisitions that seems likely to hurt startups. The UK blocked Meta’s acquisition of Gip',\n",
       " 'hy, which didn’t seem especially anticompetitive. We appreciate antitrust regulators’ efforts to create a level playing field, and we hope they’ll take a balanced approach to antitrust.Chatbot for Minority LanguagesAn AI startup that aims to crack markets in southern Asia launched a multilingual competitor to GPT-4.What’s new:\\xa0The company known as Two AI\\xa0offers\\xa0SUTRA, a low-cost language model built to be proficient in more than 30 languages, including underserved South Asian languages like Gujarati, Marath',\n",
       " 'i, Tamil, and Telugu. The company also launched\\xa0ChatSUTRA, a free-to-use web chatbot based on the model.How it works:\\xa0SUTRA comprises two mixture-of-experts transformers: a concept model and an encoder-decoder for translation. A\\xa0paper\\xa0includes some technical details, but certain details and a description of how the system fits together are either absent or ambiguous.\\xa0The concept model learned to predict the next token. The training dataset included publicly available datasets in a small number of languages ',\n",
       " 'for which abundant data is available, including English.Concurrently, the translation model learned to translate 100 million human- and machine-translated conversations among many languages. This model learned to map concepts to similar embeddings across all languages in the dataset.\\xa0The authors combined the two models, so the translation model’s encoder fed the concept model, which in turn fed the translation model’s decoder, and further trained them together. More explicitly, during this stage of training',\n",
       " ' and at inference, the translation model’s encoder receives text and produces an initial embedding. The concept model processes the embedding and delivers its output to the translation model’s decoder, which produces the resulting text.\\xa0SUTRA is available via an\\xa0API\\xa0in versions that are designated Pro (highest-performing), Light (lowest-latency), and Online (internet-connected). SUTRA-Pro and SUTRA-Online cost $1 per 1 million tokens for input and output. SUTRA-Light costs $0.75 per 1 million tokens.\\xa0Result',\n",
       " 's:\\xa0On\\xa0multilingual MMLU\\xa0(a machine-translated version of multiple-choice questions that cover a wide variety of disciplines), SUTRA outperformed GPT-4 in four of the 11 languages for which the developer reported the results: Gujarati, Marathi, Tamil, and Telugu. Moreover, SUTRA’s tokenizer is highly efficient, making the model fast and cost-effective. In key languages, it compares favorably to the tokenizer used with GPT-3.5 and GPT-4, and even narrowly outperforms GPT-4o’s improved tokenizer, according to ',\n",
       " 'Two AI’s tokenizer comparison\\xa0space\\xa0on HuggingFace. In languages such as Hindi and Korean that are written in non-Latin scripts and for which GPT-4 performs better on MMLU, SUTRA’s tokenizer generates less than half as many tokens as the one used with GPT-3.5 and GPT-4, and slightly fewer than GPT-4o’s tokenizer.Yes, but:\\xa0Multilingual MMLU tests only 11 of SUTRA’s 33 languages, making it difficult to fully evaluate the model’s multilingual performance.\\xa0Behind the news:\\xa0Two AI was founded in 2021 by Pranav M',\n",
       " 'istry, former president and CEO of Samsung Technology & Advanced Research Labs. The startup has offices in California, South Korea, and India. In 2022, it raised $20 million in seed funding from Indian telecommunications firm Jio and South Korean internet firm Naver. Mistry aims to focus on predominantly non-English-speaking markets such as India, South Korea, Japan, and the Middle East, he\\xa0told\\xa0Analytics India.Why it matters:\\xa0Many top models work in a variety of languages, but from a practical standpoint, ',\n",
       " 'multilingual models remain a frontier in natural language processing. Although SUTRA doesn’t match GPT-4 in all the languages reported, its low price and comparatively high performance may make it appealing in South Asian markets, especially rural areas where people are less likely to speak English. The languages in which SUTRA excels are spoken by tens of millions of people, and they’re the most widely spoken languages in their respective regions. Users in these places have yet to experience GPT-4-level pe',\n",
       " 'rformance in their native tongues.We’re thinking:\\xa0Can a newcomer like Two AI compete with OpenAI? If SUTRA continues to improve, or if it can maintain its cost-effective service, it may yet carve out a niche.Conversing With the DepartedAdvances in video generation have spawned a market for lifelike avatars of deceased loved ones.What’s new:\\xa0Several companies in China produce interactive videos that enable customers to chat with animated likenesses of dead friends and relatives,\\xa0MIT Technology Review\\xa0reporte',\n",
       " 'd.\\xa0How it works:\\xa0Super Brain\\xa0and\\xa0Silicon Intelligence\\xa0have built such models for several thousand customers. They provide a modern equivalent of portrait photos of deceased relatives and a vivid way to commune with ancestors.The developers use undisclosed tools to stitch photos, videos, audio recordings, and writings supplied by customers into interactive talking-head avatars of deceased loved ones.The cost has dropped dramatically. In December 2023, Super Brain charged between $1,400 and $2,800 for a basic',\n",
       " ' chat avatar wrapped in a phone app. Today it charges between $700 and $1,400 and plans eventually to drop the price to around $140. Silicon Intelligence charges between several hundred dollars for a phone-based avatar to several thousand for one displayed on a tablet.Behind the news:\\xa0The desire to interact with the dead in the form of an AI-generated avatar is neither new nor limited to China. In the U.S., the startup HereAfter AI\\xa0builds\\xa0chatbots that mimic the deceased based on interviews conducted while ',\n",
       " 'they were alive. Another startup, StoryFile, markets similar capabilities to elders (pitched\\xa0by 93-year-old\\xa0Star Trek\\xa0star William Shatner) to keep their memory alive for younger family members. The chatbot app\\xa0Replika\\xa0began as a project by founder Eugenia Kuyda to virtually resurrect a friend who perished in a car accident in 2015.\\xa0Yes, but:\\xa0In China, language models struggle with the variety of dialects spoken by many elders.Why it matters:\\xa0Virtual\\xa0newscasters\\xa0and\\xa0influencers\\xa0are increasingly visible on t',\n",
       " 'he web, but the technology has more poignant uses. People long to feel close to loved ones who are no longer present. AI can foster that sense of closeness and rapport, helping to fulfill a deep need to remember, honor, and consult the dead.We’re thinking:\\xa0No doubt, virtual avatars of the dead can bring comfort to the bereaved. But they also bring the risk that providers might manipulate their customers’ emotional attachments for profit. We urge developers to focus on strengthening relationships among livin',\n",
       " 'g family and friends.Benchmarks for Agentic BehaviorsTool use and planning are key behaviors in agentic workflows that enable large language models (LLMs) to execute complex sequences of steps. New benchmarks measure these capabilities in common workplace tasks.\\xa0What’s new:\\xa0Recent benchmarks gauge the ability of a large language model (LLM) to use external tools to manipulate corporate databases and to plan events such as travel and meetings.\\xa0Tool use:\\xa0Olly Styles, Sam Miller, and colleagues at Mindsdb, Uni',\n",
       " 'versity of Warwick, and University of Glasgow proposed\\xa0WorkBench, which tests an LLM’s ability to use 26 software tools to operate on five simulated workplace databases: email, calendar, web analytics, projects, and customer relationship management. Tools include deleting emails, looking up calendar events, creating graphs, and looking up tasks in a to-do list.The benchmark includes 690 problems that require using between zero to 12 tools to succeed. It evaluates individual examples based on whether the dat',\n",
       " 'abases changed as expected after the final tool had been called (rather than simply whether particular tools were used, as in earlier work). In this way, a model can use tools in any sequence and/or revise its initial choices if they prove unproductive and still receive credit for responding correctly.Upon receiving a problem, models are given a list of all tools and an example of how to use each one. Following the\\xa0ReAct\\xa0prompting strategy, they’re asked first to reason about the problem and then use a tool',\n",
       " '. After they’ve received a tool’s output (typically either information or an error message), they’re asked to reason again and choose another tool. The cycle of reasoning, tool selection, and receiving output repeats until the model decides it doesn’t need to use another tool.\\xa0The authors evaluated GPT-4, GPT-3.5, Claude 2, Llama2-70B, and Mixtral-8x7B. GPT-4 performed the best by a large margin: It modified the databases correctly 43 percent of the time. The closest competitor, Claude 2, modified the datab',\n",
       " 'ases correctly 26 percent of the time.Planning:\\xa0Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, and colleagues at Google published\\xa0Natural Plan, a benchmark that evaluates an LLM’s ability to (i) plan trips, (ii) arrange a series of meeting times and locations, and (iii) schedule a group meeting. Each example has only one solution.The benchmark includes 1,600 prompts that ask the model to plan a trip based on an itinerary of cities, time to be spent in each city, total duration of the trip, days when othe',\n",
       " 'r people are available to meet, and available flights between cities.\\xa01,000 prompts ask the model to plan a schedule to meet as many people as possible. The prompts include places, times when people will be in each place, and how long it takes to drive from one place to another.\\xa01,000 prompts ask the model, given the existing schedules of a number of people, to find a good time for them to meet.The authors tested GPT 3.5, GPT-4, GPT-4o, Gemini 1.5 Flash, and Gemini 1.5 Pro, using five-shot prompts (that is,',\n",
       " ' providing five examples for context). Gemini 1.5 Pro achieved the highest scores on planning trips (34.8 percent) and scheduling group meetings (48.9 percent). GPT-4 ranked second for planning trips (31.1), and GPT-4o ranked second for scheduling meetings (43.7 percent). GPT-4 dominated in arranging meetings (47 percent), followed by GPT-40 (45.2 percent).Why it matters:\\xa0When building\\xa0agentic workflows, developers must decide on LLM choices, prompting strategies, sequencing of steps to be carried out, tool',\n",
       " \" designs, single- versus multi-agent architectures, and so on. Good benchmarks can reveal which approaches work best.We're thinking:\\xa0These tests have unambiguous right answers, so agent outputs can be evaluated automatically as correct or incorrect. We look forward to further work to evaluate agents that generate free text output.\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding models --> Better leads to more meaningful/accurate embeddings\n",
    "## \"Very\" cheap\n",
    "# Large Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral-large-latest'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear friends,On Monday, a number of large music labels\\xa0sued\\xa0AI music makers Suno and Udio for copyright infringement. Their lawsuit echoes\\xa0The New York Times’\\xa0lawsuit\\xa0against OpenAI in December. The question of what’s fair when it comes to AI software remains a difficult one.\\xa0I\\xa0spoke\\xa0out in favor of OpenAI’s side in the earlier lawsuit. Humans can learn from online articles and use what they learn to produce novel works, so I’d like to be allowed to use AI to do so. Some people criticized my view as making '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(txt):\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_1 = get_text_embedding(chunk_1)\n",
    "len(embedding_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy --> 300 embeddings for md and lg models, 96 for small\n",
    "# OpenAI large embedding model --> 3072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 1024)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Make sure text_embeddings is a numpy array\n",
    "assert isinstance(text_embeddings, np.ndarray)\n",
    "\n",
    "# Add index to faiss index and cast text embedding to np.ndarray\n",
    "index = faiss.IndexFlatIP(text_embeddings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "- Train model (pre-trained model)\n",
    "- Fine-tune for specific use case\n",
    "\n",
    "Data changes...\n",
    "\n",
    "- Fine-tune again on new/different data\n",
    "- Expensive and tedious\n",
    "\n",
    "## RAG\n",
    "\n",
    "- Train model (pre-trained model)\n",
    "- Make a vector database that model could retrieve from\n",
    "\n",
    "Data changes...\n",
    "\n",
    "- Add new data to database OR remove old data if needed\n",
    "- Quick with similar accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity --> similar language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Could the use of AI be monopolistic?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])\n",
    "\n",
    "\n",
    "# sen1 = \"The cat is scared of the dog\"\n",
    "# sen2 = \"The dog is scared of the cat\"\n",
    "# sen1 != sen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02632141,  0.0491333 ,  0.02359009, ..., -0.02809143,\n",
       "         0.01449585, -0.00341988]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 10 12]]\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(question_embeddings, k=3)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 10, 12]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"monopoly\" in chunks[i] for i in range(0,len(chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hy, which didn’t seem especially anticompetitive. We appreciate antitrust regulators’ efforts to create a level playing field, and we hope they’ll take a balanced approach to antitrust.Chatbot for Minority LanguagesAn AI startup that aims to crack markets in southern Asia launched a multilingual competitor to GPT-4.What’s new:\\xa0The company known as Two AI\\xa0offers\\xa0SUTRA, a low-cost language model built to be proficient in more than 30 languages, including underserved South Asian languages like Gujarati, Marath'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mpetitive practices in search, social media, and consumer electronics. Their inquiries into the AI market could have similar impacts.\\xa0We’re thinking:\\xa0Governments must limit unfair corporate behavior without stifling legitimate activities. Recently, in the U.S. and Europe, the pendulum has swung toward overly aggressive enforcement. For example, government opposition to Adobe’s purchase of Figma had a chilling effect on acquisitions that seems likely to hurt startups. The UK blocked Meta’s acquisition of Gip', 'ncy would look for possible anti-competitive forces in the AI market.\\xa0The DOJ is concerned that Nvidia may use unfair practices to maintain its market dominance. They may look into Nvidia’s CUDA software, which strengthens users’ reliance on its chips. They may also explore\\xa0claims\\xa0raised by French authorities that Nvidia favors some cloud computing firms over others.The FTC worries that the partnership between OpenAI and Microsoft, which owns 49 percent of OpenAI and\\xa0holds\\xa0a non-voting seat on OpenAI’s boar', 'ernment attention to top AI companies is rising worldwide. Microsoft’s partnership with OpenAI\\xa0faces\\xa0additional scrutiny by European Union regulators, who are probing whether the relationship violates EU regulations that govern corporate mergers. U.K. regulators are\\xa0investigating\\xa0Amazon’s relationship with Anthropic and Microsoft’s relationship with Mistral and Inflection AI. Last year, French regulators\\xa0raided\\xa0an Nvidia office over suspected anti-competitive practices. In 2022, Nvidia\\xa0withdrew\\xa0a bid to acq']\n"
     ]
    }
   ],
   "source": [
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "print(retrieved_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "\n",
    "def mistral(user_message, model=model, is_json=False):\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, messages=messages, response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "    else:\n",
    "        chat_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the use of AI could potentially be monopolistic. The context information indicates that government authorities are looking into possible anti-competitive practices in the AI market. For instance, the DOJ is investigating Nvidia for potential unfair practices to maintain its market dominance. This includes looking into Nvidia's CUDA software, which could increase users' reliance on its chips. Additionally, the FTC is concerned about the partnership between OpenAI and Microsoft, which holds a significant stake in OpenAI. These instances suggest that the use of AI could lead to monopolistic practices. However, it's important to note that the investigations are ongoing, and no definitive conclusions have been drawn.\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer: {response}\n",
    "---------------------\n",
    "Given the answer to the question, critique how well the context was used to answer the questions accurately.\n",
    "Give 3 actionable recommendations on improving the answer to the question given the context.\n",
    "---------------------\n",
    "Critique:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer provided effectively uses the context to address the query about the potential monopolistic use of AI. It accurately identifies and explains several instances from the context where AI could be used in a monopolistic manner, such as the investigations into Nvidia and the partnership between OpenAI and Microsoft. However, there are a few areas where the answer could be improved.\n",
      "\n",
      "Recommendations for improvement:\n",
      "\n",
      "1. Provide more specific examples: While the answer does mention specific cases like the DOJ's investigation into Nvidia and the FTC's concerns about Microsoft and OpenAI, it could benefit from including more details about these cases. For instance, the answer could elaborate on how Nvidia's CUDA software could lead to monopolistic practices, or what specific aspects of the Microsoft-OpenAI partnership are causing concern.\n",
      "\n",
      "2. Address the global perspective: The context mentions investigations and concerns in the U.S., Europe, and the U.K. The answer could be improved by acknowledging this global perspective and discussing how different regions are approaching the potential monopolization of AI.\n",
      "\n",
      "3. Discuss potential consequences: The answer could be enhanced by discussing the potential impacts of these monopolistic practices on the AI market, consumers, and other stakeholders. This could provide a more comprehensive understanding of why these investigations are significant and what could happen if AI is used monopolistically.\n"
     ]
    }
   ],
   "source": [
    "critique = mistral(critique_prompt)\n",
    "print(critique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG + Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_with_context(text, question, chunk_size=512):\n",
    "    # split document into chunks\n",
    "    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    # load into a vector database\n",
    "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "    d = text_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(text_embeddings)\n",
    "    # create embeddings for a question\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # retrieve similar chunks from the vector database\n",
    "    D, I = index.search(question_embeddings, k=4)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    # generate response based on the retrieve relevant text chunks\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {retrieved_chunk}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = mistral(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 10, 12]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 10, 12]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "names_to_functions = {\"qa_with_context\": functools.partial(qa_with_context, text=text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"qa_with_context\",\n",
    "            \"description\": \"Answer user question by retrieving relevant context\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"user question\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='e889d9a86274496fbd04e9913d8653a5', object='chat.completion', created=1719757533, model='mistral-large-latest', choices=[ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content='', name=None, tool_calls=[ToolCall(id='JyI2xwlVX', type=<ToolType.function: 'function'>, function=FunctionCall(name='qa_with_context', arguments='{\"question\": \"What are the ways we can mitigate risks of monopolies introduced by AI?\"}'))], tool_call_id=None), finish_reason=<FinishReason.tool_calls: 'tool_calls'>)], usage=UsageInfo(prompt_tokens=96, total_tokens=134, completion_tokens=38))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "What are the ways we can mitigate risks of monopolies introduced by AI?\n",
    "\"\"\"\n",
    "\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "response = client.chat(\n",
    "    model=model,\n",
    "    messages=[ChatMessage(role=\"user\", content=question)],\n",
    "    tools=tools,\n",
    "    tool_choice=\"any\",\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(name='qa_with_context', arguments='{\"question\": \"What are the ways we can mitigate risks of monopolies introduced by AI?\"}')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_function = response.choices[0].message.tool_calls[0].function\n",
    "tool_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qa_with_context'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_function.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the ways we can mitigate risks of monopolies introduced by AI?'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "args = json.loads(tool_function.arguments)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, the following ways to mitigate the risks of monopolies introduced by AI are suggested:\\n\\n1. Government Regulation: The text mentions that U.S. antitrust regulators are preparing to investigate AI giants for potential monopolistic practices. This indicates that government oversight and regulation can play a significant role in mitigating the risks of AI monopolies.\\n\\n2. Limiting Unfair Corporate Behavior: The statement \"Governments must limit unfair corporate behavior without stifling legitimate activities\" suggests that preventing unfair business practices can help control AI monopolies. This could involve measures such as preventing predatory pricing, exclusivity deals, and other practices that stifle competition.\\n\\n3. Scrutiny of Mergers and Acquisitions: The text mentions several instances where regulatory bodies are probing or have blocked acquisitions and partnerships between AI companies. This suggests that careful scrutiny of mergers and acquisitions can help prevent the formation of AI monopolies.\\n\\n4. International Cooperation: The text mentions that government attention to top AI companies is rising worldwide. This suggests that international cooperation between regulators can also help in mitigating the risks of AI monopolies.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_result = names_to_functions[tool_function.name](**args)\n",
    "function_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: panel in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (1.4.4)\n",
      "Requirement already satisfied: jupyter_bokeh in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (4.0.5)\n",
      "Requirement already satisfied: bokeh<3.5.0,>=3.4.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (3.4.2)\n",
      "Requirement already satisfied: param<3.0,>=2.1.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (2.1.1)\n",
      "Requirement already satisfied: pyviz-comms>=2.0.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (3.0.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (2024.6.0)\n",
      "Requirement already satisfied: markdown in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (3.6)\n",
      "Requirement already satisfied: markdown-it-py in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (3.0.0)\n",
      "Requirement already satisfied: linkify-it-py in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (2.0.3)\n",
      "Requirement already satisfied: mdit-py-plugins in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (0.4.1)\n",
      "Requirement already satisfied: requests in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (4.66.4)\n",
      "Requirement already satisfied: bleach in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (6.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (4.12.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from panel) (2.2.2)\n",
      "Requirement already satisfied: ipywidgets==8.* in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from jupyter_bokeh) (8.1.3)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (3.1.4)\n",
      "Requirement already satisfied: contourpy>=1.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (1.26.4)\n",
      "Requirement already satisfied: packaging>=16.8 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (24.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (10.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (6.0.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (6.4.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipywidgets==8.*->jupyter_bokeh) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipywidgets==8.*->jupyter_bokeh) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipywidgets==8.*->jupyter_bokeh) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipywidgets==8.*->jupyter_bokeh) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipywidgets==8.*->jupyter_bokeh) (3.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pandas>=1.2->panel) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pandas>=1.2->panel) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pandas>=1.2->panel) (2024.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bleach->panel) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from bleach->panel) (0.5.1)\n",
      "Requirement already satisfied: uc-micro-py in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from linkify-it-py->panel) (1.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from markdown-it-py->panel) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests->panel) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests->panel) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests->panel) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests->panel) (2024.6.2)\n",
      "Requirement already satisfied: decorator in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from Jinja2>=2.9->bokeh<3.5.0,>=3.4.0->panel) (2.1.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.*->jupyter_bokeh) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install panel jupyter_bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI = User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.2.min.js\", \"https://cdn.holoviz.org/panel/1.4.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='2ac48893-e70e-4cec-8c8d-29900ce7bec4'>\n",
       "  <div id=\"c0184c0d-ec97-48f6-873e-f21609b056a4\" data-root-id=\"2ac48893-e70e-4cec-8c8d-29900ce7bec4\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"f3328849-8af6-4faf-b22d-4fda80aebf96\":{\"version\":\"3.4.2\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"2ac48893-e70e-4cec-8c8d-29900ce7bec4\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"4a098266-4d18-446b-a851-7aebd043463f\",\"attributes\":{\"plot_id\":\"2ac48893-e70e-4cec-8c8d-29900ce7bec4\",\"comm_id\":\"c1f91dfff4924b169bc5e2689bdfb126\",\"client_comm_id\":\"e1180b92dbad45bcaa48907c5bf0e848\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"f3328849-8af6-4faf-b22d-4fda80aebf96\",\"roots\":{\"2ac48893-e70e-4cec-8c8d-29900ce7bec4\":\"c0184c0d-ec97-48f6-873e-f21609b056a4\"},\"root_ids\":[\"2ac48893-e70e-4cec-8c8d-29900ce7bec4\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "2ac48893-e70e-4cec-8c8d-29900ce7bec4"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral(contents, user, chat_interface):\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    messages = [ChatMessage(role=\"user\", content=contents)]\n",
    "    chat_response = client.chat(\n",
    "        model=\"mistral-large-latest\", \n",
    "        messages=messages)\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=run_mistral, \n",
    "    callback_user=\"Mistral\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.deeplearning.ai/the-batch/issue-255/'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear friends,On Monday, a number of large music labels sued AI music makers Suno and Udio for copyright infringement. Their lawsuit echoes The New York Times’ lawsuit against OpenAI in December. The question of what’s fair when it comes to AI software remains a difficult one. I spoke out in favor of OpenAI’s side in the earlier lawsuit. Humans can learn from online articles and use what they learn to produce novel works, so I’d like to be allowed to use AI to do so. Some people criticized my view as making an unjustifiable equivalence between humans and AI. This made me realize that people have at least two views of AI: I view AI as a tool we can use and direct to our own purposes, while some people see it as akin to a separate species, distinct from us, with its own goals and desires.If I’m allowed to build a house, I want to be allowed to use a hammer, saw, drill, or any other tool that might get the job done efficiently. If I’m allowed to read a webpage, I’d like to be allowed to read it with any web browser, and perhaps even have the browser modify the page’s formatting for accessibility. More generally, if we agree that humans are allowed to do certain things — such as read and synthesize information on the web — then my inclination is to let humans direct AI to automate this task. In contrast to this view of AI as a tool, if someone thinks humans and AI are akin to separate species, they’ll frame the question differently. Few people today think all species should have identical rights. If a mosquito annoys a human, the mosquito can be evicted (or worse). In this view, there’s no reason to think that, just because humans are allowed to do something, AI should be allowed to do it as well. To be clear, just as humans aren’t allowed to reproduce large parts of copyrighted works verbatim (or nearly verbatim) without permission, AI shouldn’t be allowed to do so either. The lawsuit against Suno and Udio points out that, when prompted in a particular way, these services can nearly reproduce pieces of copyrighted music.But here, too, there are complex issues. If someone were to use a public cloud to distribute online content in violation of copyright, typically the person who did that would be at fault, not the cloud company (so long as the company took reasonable precautions and didn’t enable copyright infringement deliberately). The plaintiffs in the lawsuit against Suno and Udio managed to write prompts that caused the systems to reproduce copyrighted work. But is this like someone managing to get a public cloud to scrape and distribute content in a way that violates copyright? Or is this — as OpenAI said — a rare bug that AI companies are working to eliminate? (Disclaimer: I’m not a lawyer and I’m not giving legal advice.)Humans and software systems use very different mechanisms for processing information. So in terms of what humans can do — and thus what I’d like to be allowed to use software to help me do — it’s helpful to consider the inputs and outputs. Specifically, if I’m allowed to listen to a lot of music and then compose a novel piece of music, I would like to be allowed to use AI to implement a similar input-to-output mapping. The process for implementing this mapping may be training a neural network on music that’s legally published on the open internet for people to enjoy without encumbrances.To acknowledge a weakness of my argument, just because humans are allowed to emit a few pounds of carbon dioxide per day simply by breathing doesn’t mean we should allow machines to emit massively more carbon dioxide without restrictions. Scale can change the nature of an act. When I was a high-school student in an internship job, I spent numerous hours photocopying, and I remember wishing I could automate that repetitive work. Humans do lots of valuable work, and AI, used as a tool to automate what we do, will create lots of value. I hope we can empower people to use tools to automate activities they’re allowed to do, and erect barriers to this only in extraordinary circumstances, when we have clear evidence that it creates more harm than benefit to society.Keep learning!Andrew A MESSAGE FROM DEEPLEARNING.AILearn to reduce the carbon footprints of your AI projects in “Carbon Aware Computing for GenAI Developers,” a new course built in collaboration with Google Cloud. Perform model training and inference jobs with cleaner, low-carbon energy and make your AI development greener! Join todayNewsU.S. to Probe AI Monopoly ConcernsU.S. antitrust regulators are preparing to investigate a trio of AI giants.What’s new: Two government agencies responsible for enforcing United States anti-monopoly laws agreed to investigate Microsoft, Nvidia, and OpenAI, The New York Times reported. How it works: The Department of Justice (DOJ) will investigate Nvidia, which dominates the market for chips that train and run neural networks. The Federal Trade Commission (FTC) will probe Microsoft and its relationship with OpenAI, which together control the distribution of OpenAI’s popular GPT-series models. In February, FTC chair Lina Khan said the agency would look for possible anti-competitive forces in the AI market. The DOJ is concerned that Nvidia may use unfair practices to maintain its market dominance. They may look into Nvidia’s CUDA software, which strengthens users’ reliance on its chips. They may also explore claims raised by French authorities that Nvidia favors some cloud computing firms over others.The FTC worries that the partnership between OpenAI and Microsoft, which owns 49 percent of OpenAI and holds a non-voting seat on OpenAI’s board of directors, may work to limit consumer choice. Microsoft’s April agreement with Inflection AI to hire most of its staff in return for a $650 million payment, which resembled an acquisition but left Inflection’s corporate structure intact, raised suspicions that the deal had been structured to avoid automatic antitrust scrutiny. The FTC previously investigated investments in Anthropic by Amazon and Google as well as whether OpenAI gathered training data in ways that harmed consumers. Behind the news: Government attention to top AI companies is rising worldwide. Microsoft’s partnership with OpenAI faces additional scrutiny by European Union regulators, who are probing whether the relationship violates EU regulations that govern corporate mergers. U.K. regulators are investigating Amazon’s relationship with Anthropic and Microsoft’s relationship with Mistral and Inflection AI. Last year, French regulators raided an Nvidia office over suspected anti-competitive practices. In 2022, Nvidia withdrew a bid to acquire chip designer Arm Holdings after the proposal attracted international regulatory scrutiny including an FTC lawsuit.Why it matters: Microsoft, Nvidia, and OpenAI have put tens of billions of dollars each into the AI market, and lawsuits, settlements, judgments, or other interventions could shape the fate of those investments. The FTC and DOJ similarly divided their jurisdictions in 2019, resulting in investigations into — and ongoing lawsuits against — Amazon, Apple, Google, and Meta for alleged anti-competitive practices in search, social media, and consumer electronics. Their inquiries into the AI market could have similar impacts. We’re thinking: Governments must limit unfair corporate behavior without stifling legitimate activities. Recently, in the U.S. and Europe, the pendulum has swung toward overly aggressive enforcement. For example, government opposition to Adobe’s purchase of Figma had a chilling effect on acquisitions that seems likely to hurt startups. The UK blocked Meta’s acquisition of Giphy, which didn’t seem especially anticompetitive. We appreciate antitrust regulators’ efforts to create a level playing field, and we hope they’ll take a balanced approach to antitrust.Chatbot for Minority LanguagesAn AI startup that aims to crack markets in southern Asia launched a multilingual competitor to GPT-4.What’s new: The company known as Two AI offers SUTRA, a low-cost language model built to be proficient in more than 30 languages, including underserved South Asian languages like Gujarati, Marathi, Tamil, and Telugu. The company also launched ChatSUTRA, a free-to-use web chatbot based on the model.How it works: SUTRA comprises two mixture-of-experts transformers: a concept model and an encoder-decoder for translation. A paper includes some technical details, but certain details and a description of how the system fits together are either absent or ambiguous. The concept model learned to predict the next token. The training dataset included publicly available datasets in a small number of languages for which abundant data is available, including English.Concurrently, the translation model learned to translate 100 million human- and machine-translated conversations among many languages. This model learned to map concepts to similar embeddings across all languages in the dataset. The authors combined the two models, so the translation model’s encoder fed the concept model, which in turn fed the translation model’s decoder, and further trained them together. More explicitly, during this stage of training and at inference, the translation model’s encoder receives text and produces an initial embedding. The concept model processes the embedding and delivers its output to the translation model’s decoder, which produces the resulting text. SUTRA is available via an API in versions that are designated Pro (highest-performing), Light (lowest-latency), and Online (internet-connected). SUTRA-Pro and SUTRA-Online cost $1 per 1 million tokens for input and output. SUTRA-Light costs $0.75 per 1 million tokens. Results: On multilingual MMLU (a machine-translated version of multiple-choice questions that cover a wide variety of disciplines), SUTRA outperformed GPT-4 in four of the 11 languages for which the developer reported the results: Gujarati, Marathi, Tamil, and Telugu. Moreover, SUTRA’s tokenizer is highly efficient, making the model fast and cost-effective. In key languages, it compares favorably to the tokenizer used with GPT-3.5 and GPT-4, and even narrowly outperforms GPT-4o’s improved tokenizer, according to Two AI’s tokenizer comparison space on HuggingFace. In languages such as Hindi and Korean that are written in non-Latin scripts and for which GPT-4 performs better on MMLU, SUTRA’s tokenizer generates less than half as many tokens as the one used with GPT-3.5 and GPT-4, and slightly fewer than GPT-4o’s tokenizer.Yes, but: Multilingual MMLU tests only 11 of SUTRA’s 33 languages, making it difficult to fully evaluate the model’s multilingual performance. Behind the news: Two AI was founded in 2021 by Pranav Mistry, former president and CEO of Samsung Technology & Advanced Research Labs. The startup has offices in California, South Korea, and India. In 2022, it raised $20 million in seed funding from Indian telecommunications firm Jio and South Korean internet firm Naver. Mistry aims to focus on predominantly non-English-speaking markets such as India, South Korea, Japan, and the Middle East, he told Analytics India.Why it matters: Many top models work in a variety of languages, but from a practical standpoint, multilingual models remain a frontier in natural language processing. Although SUTRA doesn’t match GPT-4 in all the languages reported, its low price and comparatively high performance may make it appealing in South Asian markets, especially rural areas where people are less likely to speak English. The languages in which SUTRA excels are spoken by tens of millions of people, and they’re the most widely spoken languages in their respective regions. Users in these places have yet to experience GPT-4-level performance in their native tongues.We’re thinking: Can a newcomer like Two AI compete with OpenAI? If SUTRA continues to improve, or if it can maintain its cost-effective service, it may yet carve out a niche.Conversing With the DepartedAdvances in video generation have spawned a market for lifelike avatars of deceased loved ones.What’s new: Several companies in China produce interactive videos that enable customers to chat with animated likenesses of dead friends and relatives, MIT Technology Review reported. How it works: Super Brain and Silicon Intelligence have built such models for several thousand customers. They provide a modern equivalent of portrait photos of deceased relatives and a vivid way to commune with ancestors.The developers use undisclosed tools to stitch photos, videos, audio recordings, and writings supplied by customers into interactive talking-head avatars of deceased loved ones.The cost has dropped dramatically. In December 2023, Super Brain charged between $1,400 and $2,800 for a basic chat avatar wrapped in a phone app. Today it charges between $700 and $1,400 and plans eventually to drop the price to around $140. Silicon Intelligence charges between several hundred dollars for a phone-based avatar to several thousand for one displayed on a tablet.Behind the news: The desire to interact with the dead in the form of an AI-generated avatar is neither new nor limited to China. In the U.S., the startup HereAfter AI builds chatbots that mimic the deceased based on interviews conducted while they were alive. Another startup, StoryFile, markets similar capabilities to elders (pitched by 93-year-old Star Trek star William Shatner) to keep their memory alive for younger family members. The chatbot app Replika began as a project by founder Eugenia Kuyda to virtually resurrect a friend who perished in a car accident in 2015. Yes, but: In China, language models struggle with the variety of dialects spoken by many elders.Why it matters: Virtual newscasters and influencers are increasingly visible on the web, but the technology has more poignant uses. People long to feel close to loved ones who are no longer present. AI can foster that sense of closeness and rapport, helping to fulfill a deep need to remember, honor, and consult the dead.We’re thinking: No doubt, virtual avatars of the dead can bring comfort to the bereaved. But they also bring the risk that providers might manipulate their customers’ emotional attachments for profit. We urge developers to focus on strengthening relationships among living family and friends.Benchmarks for Agentic BehaviorsTool use and planning are key behaviors in agentic workflows that enable large language models (LLMs) to execute complex sequences of steps. New benchmarks measure these capabilities in common workplace tasks. What’s new: Recent benchmarks gauge the ability of a large language model (LLM) to use external tools to manipulate corporate databases and to plan events such as travel and meetings. Tool use: Olly Styles, Sam Miller, and colleagues at Mindsdb, University of Warwick, and University of Glasgow proposed WorkBench, which tests an LLM’s ability to use 26 software tools to operate on five simulated workplace databases: email, calendar, web analytics, projects, and customer relationship management. Tools include deleting emails, looking up calendar events, creating graphs, and looking up tasks in a to-do list.The benchmark includes 690 problems that require using between zero to 12 tools to succeed. It evaluates individual examples based on whether the databases changed as expected after the final tool had been called (rather than simply whether particular tools were used, as in earlier work). In this way, a model can use tools in any sequence and/or revise its initial choices if they prove unproductive and still receive credit for responding correctly.Upon receiving a problem, models are given a list of all tools and an example of how to use each one. Following the ReAct prompting strategy, they’re asked first to reason about the problem and then use a tool. After they’ve received a tool’s output (typically either information or an error message), they’re asked to reason again and choose another tool. The cycle of reasoning, tool selection, and receiving output repeats until the model decides it doesn’t need to use another tool. The authors evaluated GPT-4, GPT-3.5, Claude 2, Llama2-70B, and Mixtral-8x7B. GPT-4 performed the best by a large margin: It modified the databases correctly 43 percent of the time. The closest competitor, Claude 2, modified the databases correctly 26 percent of the time.Planning: Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, and colleagues at Google published Natural Plan, a benchmark that evaluates an LLM’s ability to (i) plan trips, (ii) arrange a series of meeting times and locations, and (iii) schedule a group meeting. Each example has only one solution.The benchmark includes 1,600 prompts that ask the model to plan a trip based on an itinerary of cities, time to be spent in each city, total duration of the trip, days when other people are available to meet, and available flights between cities. 1,000 prompts ask the model to plan a schedule to meet as many people as possible. The prompts include places, times when people will be in each place, and how long it takes to drive from one place to another. 1,000 prompts ask the model, given the existing schedules of a number of people, to find a good time for them to meet.The authors tested GPT 3.5, GPT-4, GPT-4o, Gemini 1.5 Flash, and Gemini 1.5 Pro, using five-shot prompts (that is, providing five examples for context). Gemini 1.5 Pro achieved the highest scores on planning trips (34.8 percent) and scheduling group meetings (48.9 percent). GPT-4 ranked second for planning trips (31.1), and GPT-4o ranked second for scheduling meetings (43.7 percent). GPT-4 dominated in arranging meetings (47 percent), followed by GPT-40 (45.2 percent).Why it matters: When building agentic workflows, developers must decide on LLM choices, prompting strategies, sequencing of steps to be carried out, tool designs, single- versus multi-agent architectures, and so on. Good benchmarks can reveal which approaches work best.We're thinking: These tests have unambiguous right answers, so agent outputs can be evaluated automatically as correct or incorrect. We look forward to further work to evaluate agents that generate free text output.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "response = requests.get(\n",
    "    # \"https://www.deeplearning.ai/the-batch/a-roadmap-explores-how-ai-can-detect-and-mitigate-greenhouse-gases/\"\n",
    "    URL\n",
    ")\n",
    "html_doc = response.text\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "tag = soup.find(\"div\", re.compile(\"^prose--styled\"))\n",
    "text = tag.text\n",
    "print(text)\n",
    "\n",
    "# Optionally save this text into a file.\n",
    "file_name = \"AI_greenhouse_gas.txt\"\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "prompt = \"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=input)\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "\n",
    "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "    chat_response = client.chat(model=model, messages=messages)\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "def answer_question(question, user, instance):\n",
    "    text = file_input.value.decode(\"utf-8\")\n",
    "\n",
    "    # split document into chunks\n",
    "    chunk_size = 1024\n",
    "    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    # load into a vector database\n",
    "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "    d = text_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(text_embeddings)\n",
    "    # create embeddings for a question\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # retrieve similar chunks from the vector database\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    # generate response based on the retrieved relevant text chunks\n",
    "    response = run_mistral(\n",
    "        prompt.format(retrieved_chunk=retrieved_chunk, question=question)\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the Chat interface with hour user-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a3611273ff4357a6ae35e28037456e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'51f71d79-e37d-4ce3-84f1-e6795de61fe2': {'version…"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_input = pn.widgets.FileInput(accept=\".txt\", value=\"\", height=50)\n",
    "\n",
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=answer_question,\n",
    "    callback_user=\"Mistral\",\n",
    "    header=pn.Row(file_input, \"### Upload a text file to chat with it!\"),\n",
    ")\n",
    "chat_interface.send(\n",
    "    \"Send a message to get a reply from Mistral!\", \n",
    "    user=\"System\", \n",
    "    respond=False\n",
    ")\n",
    "chat_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa82d9e202814b12b52fabd49832d9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'df4ec31f-c970-40b6-8c29-88925e61e6ca': {'version…"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_input = pn.widgets.FileInput(accept=\".txt\", value=\"\", height=50)\n",
    "\n",
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=answer_question,\n",
    "    callback_user=\"Mistral\",\n",
    "    header=pn.Row(file_input, \"### Upload a text file to chat with it!\"),\n",
    ")\n",
    "chat_interface.send(\n",
    "    \"Send a message to get a reply from Mistral!\", \n",
    "    user=\"System\", \n",
    "    respond=False\n",
    ")\n",
    "chat_interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying LLMs (Prompt (few-shot, one-shot, many-shot + many other techniques in prompt engineering) --> LLM --> Output)\n",
    "- AWS\n",
    "- Google Cloud (my preference)\n",
    "    - Use Mistral through Vertex AI\n",
    "    - Create a cloud function that can be called by it's endpoint (this is like your own API)\n",
    "        * Prompt engineering\n",
    "        * Embeddings work + when to embed...\n",
    "    - Store the vector database in google cloud storage\n",
    "    - Either\n",
    "        * Build own interface (REACT + great UI/UX)\n",
    "        * Call the endpoint to your GCloud function and ta da :)\n",
    "    - OR\n",
    "        * Create some sort of add-on on your chosen platform\n",
    "        * Google excel, sheets, etc.\n",
    "- Some other\n",
    "\n",
    "# Deploying traditional models (Data --> Solvable task - regression or classification, for example --> accurate answer)\n",
    "- Example: Iris dataset. Input ['1.56', '2.45', '7.5', '1.2'] for ['sepal length', 'sepal width', 'petal lenght', 'petal width']\n",
    "    * Ask user to input specific metrics, not nuanced prompt.\n",
    "- Output the most correct answer based on our model's learned relationship.\n",
    "- Google Cloud\n",
    "    - Interface asking specific metrics/photo/audio\n",
    "        * Classify dog/cat/zebra/etc.\n",
    "    - Gcloud function specifying logic --> will use the model and pass in the input to get an output\n",
    "    - return output when function endpoint is called\n",
    "    - display answer to user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canva --> stunning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
