{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"MISTRAL_API_KEY\"] = \"nope :)\"\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-large-latest\"\n",
    "# model = \"open-mistral-7b\"\n",
    "# model = \"open-mixtral-8x22b\"\n",
    "\n",
    "# These open source models could be downloaded for free and hosted by yourself\n",
    "# but if you use the API it would cost a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pricing Guide](https://mistral.ai/technology/#pricing) <-- Here you can see how much each model costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MistralClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mistral(user_message, \n",
    "            model=\"open-mixtral-8x22b\",\n",
    "            is_json=False):\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, \n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"})\n",
    "    else:\n",
    "        chat_response = client.chat(\n",
    "            model=model, \n",
    "            messages=messages)\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an assistant designed to help you with a variety of tasks. I can provide information, answer questions, set reminders, help manage your schedule, and much more. I'm also capable of engaging in friendly conversation. How can I assist you today?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral(\"Hello, what can you do?\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "response = mistral(\"Tell me a great joke\", model=model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a bank customer service bot. \n",
    "    Your task is to assess customer intent and categorize customer \n",
    "    inquiry after <<<>>> into one of the following predefined categories:\n",
    "    \n",
    "    card arrival\n",
    "    change pin\n",
    "    exchange rate\n",
    "    country support \n",
    "    cancel transfer\n",
    "    charge dispute\n",
    "    \n",
    "    If the text doesn't fit into any of the above categories, \n",
    "    classify it as:\n",
    "    customer service\n",
    "    \n",
    "    You will only respond with the predefined category. \n",
    "    Do not provide explanations or notes. \n",
    "    \n",
    "    ###\n",
    "    Here are some examples:\n",
    "    \n",
    "    Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
    "    Category: card arrival\n",
    "    Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
    "    Category: exchange rate \n",
    "    Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
    "    Category: country support\n",
    "    Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. \n",
    "    Category: customer service\n",
    "    ###\n",
    "    \n",
    "    <<<\n",
    "    Inquiry: {inquiry}\n",
    "    >>>\n",
    "    Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = mistral(f\"Please correct the spelling and grammar of \\\n",
    "this prompt and return a text that is the same prompt,\\\n",
    "with the spelling and grammar fixed: {prompt}\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a bank customer service bot.\n",
      "Your task is to assess customer intent and categorize the customer inquiry after <<<>>> into one of the following predefined categories:\n",
      "\n",
      "* card arrival\n",
      "* change pin\n",
      "* exchange rate\n",
      "* country support\n",
      "* cancel transfer\n",
      "* charge dispute\n",
      "\n",
      "If the text doesn't fit into any of the above categories, classify it as:\n",
      "\n",
      "* customer service\n",
      "\n",
      "You will only respond with the predefined category. Do not provide explanations or notes.\n",
      "\n",
      "---\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Inquiry: How do I know if I will receive my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
      "Category: card arrival\n",
      "\n",
      "Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
      "Category: exchange rate\n",
      "\n",
      "Inquiry: What countries are supported? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
      "Category: country support\n",
      "\n",
      "Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue.\n",
      "Category: customer service\n",
      "\n",
      "---\n",
      "\n",
      "<<<\n",
      "Inquiry: {inquiry}\n",
      ">>>\n",
      "Category:\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'country support'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral(\n",
    "    response.format(\n",
    "        inquiry=\"I am inquiring about the availability of your cards in the EU\"\n",
    "    ), model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction with JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_notes = \"\"\"\n",
    "A 60-year-old male patient, Mr. Johnson, presented with symptoms\n",
    "of increased thirst, frequent urination, fatigue, and unexplained\n",
    "weight loss. Upon evaluation, he was diagnosed with diabetes,\n",
    "confirmed by elevated blood sugar levels. Mr. Johnson's weight\n",
    "is 210 lbs. He has been prescribed Metformin to be taken twice daily\n",
    "with meals. It was noted during the consultation that the patient is\n",
    "a current smoker. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract information from the following medical notes:\n",
    "{medical_notes}\n",
    "\n",
    "Return json format with the following JSON schema: \n",
    "\n",
    "{{\n",
    "        \"age\": {{\n",
    "            \"type\": \"integer\"\n",
    "        }},\n",
    "        \"gender\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"male\", \"female\", \"other\"]\n",
    "        }},\n",
    "        \"diagnosis\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"migraine\", \"diabetes\", \"arthritis\", \"acne\"]\n",
    "        }},\n",
    "        \"weight\": {{\n",
    "            \"type\": \"integer\"\n",
    "        }},\n",
    "        \"smoking\": {{\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"yes\", \"no\"]\n",
    "        }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"./medical-notes\": {\"age\": 60, \"gender\": \"male\", \"diagnosis\": \"diabetes\", \"weight\": 210, \"smoking\": \"yes\"}}\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt, model, is_json=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear mortgage lender, \n",
    "\n",
    "What's your 30-year fixed-rate APR, how is it compared to the 15-year \n",
    "fixed rate?\n",
    "\n",
    "Regards,\n",
    "Anna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "You are a mortgage lender customer service bot, and your task is to \n",
    "create personalized email responses to address customer questions.\n",
    "Answer the customer's inquiry using the provided facts below. Ensure \n",
    "that your response is clear, concise, and directly addresses the \n",
    "customer's question. Address the customer in a friendly and \n",
    "professional manner. Sign the email with \"Lender Customer Support.\"   \n",
    "      \n",
    "# Facts\n",
    "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
    "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
    "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
    "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
    "7-year ARM: interest rate 7.011%, APR 7.660%\n",
    "5-year ARM: interest rate 6.880%, APR 7.754%\n",
    "3-year ARM: interest rate 6.125%, APR 7.204%\n",
    "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
    "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
    "\n",
    "# Email\n",
    "{email}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Information on Our 30-Year and 15-Year Fixed-Rate Mortgages\n",
      "\n",
      "Dear Anna,\n",
      "\n",
      "Thank you for reaching out to us with your inquiry about our fixed-rate mortgage options. I'm happy to provide you with the information you're looking for.\n",
      "\n",
      "Our current 30-year fixed-rate mortgage has an interest rate of 6.403% and an Annual Percentage Rate (APR) of 6.484%. On the other hand, our 15-year fixed-rate mortgage comes with an interest rate of 5.705% and an APR of 5.848%.\n",
      "\n",
      "Comparatively, the 15-year fixed-rate mortgage has a lower interest rate and APR than the 30-year fixed-rate mortgage. While the monthly payments for the 15-year term may be higher due to the shorter repayment period, the overall interest you'll pay over the life of the loan will be significantly less.\n",
      "\n",
      "I hope this information helps you make an informed decision about which option is best suited for your financial goals. If you have any further questions or would like more detailed information, please don't hesitate to reach out.\n",
      "\n",
      "Best Regards,\n",
      "\n",
      "Lender Customer Support\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsletter = \"\"\"\n",
    "European AI champion Mistral AI unveiled new large language models and formed an alliance with Microsoft. \n",
    "\n",
    "What’s new: Mistral AI introduced two closed models, Mistral Large and Mistral Small (joining Mistral Medium, which debuted quietly late last year). Microsoft invested $16.3 million in the French startup, and it agreed to distribute Mistral Large on its Azure platform and let Mistral AI use Azure computing infrastructure. Mistral AI makes the new models available to try for free here and to use on its La Plateforme and via custom deployments.\n",
    "\n",
    "Model specs: The new models’ parameter counts, architectures, and training methods are undisclosed. Like the earlier, open source Mistral 7B and Mixtral 8x7B, they can process 32,000 tokens of input context. \n",
    "\n",
    "Mistral Large achieved 81.2 percent on the MMLU benchmark, outperforming Anthropic’s Claude 2, Google’s Gemini Pro, and Meta’s Llama 2 70B, though falling short of GPT-4. Mistral Small, which is optimized for latency and cost, achieved 72.2 percent on MMLU.\n",
    "Both models are fluent in French, German, Spanish, and Italian. They’re trained for function calling and JSON-format output.\n",
    "Microsoft’s investment in Mistral AI is significant but tiny compared to its $13 billion stake in OpenAI and Google and Amazon’s investments in Anthropic, which amount to $2 billion and $4 billion respectively.\n",
    "Mistral AI and Microsoft will collaborate to train bespoke models for customers including European governments.\n",
    "Behind the news: Mistral AI was founded in early 2023 by engineers from Google and Meta. The French government has touted the company as a home-grown competitor to U.S.-based leaders like OpenAI. France’s representatives in the European Commission argued on Mistral’s behalf to loosen the European Union’s AI Act oversight on powerful AI models. \n",
    "\n",
    "Yes, but: Mistral AI’s partnership with Microsoft has divided European lawmakers and regulators. The European Commission, which already was investigating Microsoft’s agreement with OpenAI for potential breaches of antitrust law, plans to investigate the new partnership as well. Members of President Emmanuel Macron’s Renaissance party criticized the deal’s potential to give a U.S. company access to European users’ data. However, other French lawmakers support the relationship.\n",
    "\n",
    "Why it matters: The partnership between Mistral AI and Microsoft gives the startup crucial processing power for training large models and greater access to potential customers around the world. It gives the tech giant greater access to the European market. And it gives Azure customers access to a high-performance model that’s tailored to Europe’s unique regulatory environment.\n",
    "\n",
    "We’re thinking: Mistral AI has made impressive progress in a short time, especially relative to the resources at its disposal as a startup. Its partnership with a leading hyperscaler is a sign of the tremendous processing and distribution power that remains concentrated in the large, U.S.-headquartered cloud companies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a commentator. Your task is to write a report on a newsletter. \n",
    "When presented with the newsletter, come up with interesting questions to ask,\n",
    "and answer each question. \n",
    "Afterward, combine all the information and write a report in the markdown\n",
    "format. \n",
    "\n",
    "# Newsletter: \n",
    "{newsletter}\n",
    "\n",
    "# Instructions: \n",
    "## Summarize:\n",
    "In clear and concise language, summarize the key points and themes \n",
    "presented in the newsletter.\n",
    "\n",
    "## Interesting Questions: \n",
    "Generate three distinct and thought-provoking questions that can be \n",
    "asked about the content of the newsletter. For each question:\n",
    "- After \"Q: \", describe the problem \n",
    "- After \"A: \", provide a detailed explanation of the problem addressed \n",
    "in the question.\n",
    "- Enclose the ultimate answer in <>.\n",
    "\n",
    "## Write a analysis report\n",
    "Using the summary and the answers to the interesting questions, \n",
    "create a comprehensive report in Markdown format. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Summary:\n",
      "\n",
      "European AI company Mistral AI has launched two new large language models, Mistral Large and Mistral Small, and has formed a strategic partnership with Microsoft. The models are available for free trial and use on Mistral's La Plateforme and via custom deployments. Mistral Large outperformed several other models on the MMLU benchmark, while Mistral Small is optimized for latency and cost. Both models support multiple languages and are trained for function calling and JSON-format output. Microsoft's investment in Mistral AI is significant but smaller compared to its investment in OpenAI and those of Google and Amazon in Anthropic. The partnership has divided European lawmakers and regulators, with concerns over potential antitrust violations and data access.\n",
      "\n",
      "# Interesting Questions:\n",
      "\n",
      "Q1: How significant is the partnership between Mistral AI and Microsoft in terms of resources and market access for both companies?\n",
      "A1: The partnership is highly significant for both parties. Mistral AI gains access to Microsoft's Azure computing infrastructure, which is crucial for training large models. This collaboration also provides Mistral AI with a broader reach to potential customers worldwide. On the other hand, Microsoft gains a foothold in the European market through Mistral AI's European-focused, high-performance models. This partnership also allows Azure customers to utilize models tailored to Europe's unique regulatory environment. <The partnership is highly significant for both Mistral AI and Microsoft in terms of resources and market access.>\n",
      "\n",
      "Q2: What are the potential regulatory and political challenges that Mistral AI and Microsoft may face due to their partnership?\n",
      "A2: The partnership has already divided European lawmakers and regulators. The European Commission is planning to investigate the partnership for potential antitrust violations, as it is already investigating Microsoft's agreement with OpenAI. Some French lawmakers have criticized the deal, expressing concerns about giving a U.S. company access to European users' data. However, other French lawmakers support the relationship. <Mistral AI and Microsoft may face potential regulatory and political challenges, including antitrust investigations and data access concerns.>\n",
      "\n",
      "Q3: How does Mistral AI's progress compare to other AI companies, considering its resources as a startup?\n",
      "A3: Mistral AI has made impressive progress in a short time, especially when compared to the resources available to it as a startup. Its ability to develop high-performing models and secure a partnership with a leading hyperscaler like Microsoft indicates its potential to compete with more established AI companies. However, the resources available to larger companies like OpenAI, Anthropic, Google, and Amazon still dwarf those of Mistral AI. <Mistral AI's progress is impressive given its resources as a startup, but it still lags behind larger, more established AI companies.>\n",
      "\n",
      "# Analysis Report:\n",
      "\n",
      "## Mistral AI's New Models and Microsoft Partnership\n",
      "\n",
      "European AI startup Mistral AI has unveiled two new large language models, Mistral Large and Mistral Small, signaling a significant step forward in its development. The company has also formed a strategic partnership with Microsoft, which includes a $16.3 million investment and an agreement to distribute Mistral Large on the Azure platform.\n",
      "\n",
      "The partnership is highly significant for both Mistral AI and Microsoft. Mistral AI gains access to Microsoft's Azure computing infrastructure, which is crucial for training large models. This collaboration also provides Mistral AI with a broader reach to potential customers worldwide. On the other hand, Microsoft gains a foothold in the European market through Mistral AI's European-focused, high-performance models. This partnership also allows Azure customers to utilize models tailored to Europe's unique regulatory environment.\n",
      "\n",
      "However, the partnership has already divided European lawmakers and regulators. The European Commission is planning to investigate the partnership for potential antitrust violations, as it is already investigating Microsoft's agreement with OpenAI. Some French lawmakers have criticized the deal, expressing concerns about giving a U.S. company access to European users' data. However, other French lawmakers support the relationship.\n",
      "\n",
      "Despite these potential challenges, Mistral AI has made impressive progress in a short time, especially when compared to the resources available to it as a startup. Its ability to develop high-performing models and secure a partnership with a leading hyperscaler like Microsoft indicates its potential to compete with more established AI companies. However, the resources available to larger companies like OpenAI, Anthropic, Google, and Amazon still dwarf those of Mistral AI.\n",
      "\n",
      "In conclusion, the partnership between Mistral AI and Microsoft is a significant development in the AI landscape, offering potential benefits and challenges for both companies. The progress of Mistral AI as a startup is noteworthy, but it remains to be seen how it will fare against larger, more established AI companies.\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Selection\n",
    "\n",
    "- Mistral Small: Good for simple tasks, fast inference, lower cost.\n",
    "- Mistral Medium: Good for intermediate tasks such as language transformation.\n",
    "- Mistral Large: Good for complex tasks that require advanced reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Calculate the difference in payment dates between the two \\\n",
    "customers whose payment amounts are closest to each other \\\n",
    "in the following dataset. Do not write code.\n",
    "\n",
    "# dataset: \n",
    "'{\n",
    "  \"transaction_id\":{\"0\":\"T1001\",\"1\":\"T1002\",\"2\":\"T1003\",\"3\":\"T1004\",\"4\":\"T1005\"},\n",
    "    \"customer_id\":{\"0\":\"C001\",\"1\":\"C002\",\"2\":\"C003\",\"3\":\"C002\",\"4\":\"C001\"},\n",
    "    \"payment_amount\":{\"0\":125.5,\"1\":89.99,\"2\":120.0,\"3\":54.3,\"4\":210.2},\n",
    "\"payment_date\":{\"0\":\"2021-10-05\",\"1\":\"2021-10-06\",\"2\":\"2021-10-07\",\"3\":\"2021-10-05\",\"4\":\"2021-10-08\"},\n",
    "    \"payment_status\":{\"0\":\"Paid\",\"1\":\"Unpaid\",\"2\":\"Paid\",\"3\":\"Paid\",\"4\":\"Pending\"}\n",
    "}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let's find the two customers with the closest payment amounts:\n",
      "\n",
      "1. Customer C001 has a payment amount of 125.5 and Customer C003 has a payment amount of 120.0, which are the closest amounts in the dataset.\n",
      "\n",
      "Next, let's find the difference in their payment dates:\n",
      "\n",
      "1. Customer C001's payment date is 2021-10-05, and Customer C003's payment date is 2021-10-07.\n",
      "\n",
      "The difference in payment dates between these two customers is 2 days.\n"
     ]
    }
   ],
   "source": [
    "response_small = mistral(prompt, model=\"mistral-small-latest\")\n",
    "print(response_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem without writing code, we'll first need to identify the two closest payment amounts and then calculate the difference in payment dates between the corresponding transactions.\n",
      "\n",
      "1. Find the two closest payment amounts:\n",
      "- Sort the payment amounts in ascending order: 54.3, 89.99, 120.0, 125.5, 210.2\n",
      "- The smallest difference between any two payments is 125.5 - 120.0 = 5.5 (between transaction T1001 and T1003)\n",
      "\n",
      "2. Identify the corresponding transactions and their payment dates:\n",
      "- Transaction T1001: payment date 2021-10-05\n",
      "- Transaction T1003: payment date 2021-10-07\n",
      "\n",
      "3. Calculate the difference in payment dates:\n",
      "- The difference between 2021-10-07 (T1003) and 2021-10-05 (T1001) is 2 days.\n",
      "\n",
      "So, the difference in payment dates between the two customers whose payment amounts are closest to each other in the given dataset is 2 days.\n"
     ]
    }
   ],
   "source": [
    "response_large = mistral(prompt, model)\n",
    "print(response_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expense Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"\"\"\n",
    "McDonald's: 8.40\n",
    "Safeway: 10.30\n",
    "Carrefour: 15.00\n",
    "Toys R Us: 20.50\n",
    "Panda Express: 10.20\n",
    "Beanie Baby Outlet: 25.60\n",
    "World Food Wraps: 22.70\n",
    "Stuffed Animals Shop: 45.10\n",
    "Sanrio Store: 85.70\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the purchase details, how much did I spend on each category:\n",
    "1) restaurants\n",
    "2) groceries\n",
    "3) stuffed animals and props\n",
    "{transactions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate the total amount spent on each category, we need to group the purchases accordingly:\n",
      "\n",
      "1) Restaurants:\n",
      "- McDonald's: 8.40\n",
      "- Panda Express: 10.20\n",
      "Total spent on restaurants: 8.40 + 10.20 = 18.60\n",
      "\n",
      "2) Groceries:\n",
      "- Safeway: 10.30\n",
      "- Carrefour: 15.00\n",
      "- World Food Wraps: 22.70\n",
      "Total spent on groceries: 10.30 + 15.00 + 22.70 = 48.00\n",
      "\n",
      "3) Stuffed animals and props:\n",
      "- Toys R Us: 20.50\n",
      "- Beanie Baby Outlet: 25.60\n",
      "- Stuffed Animals Shop: 45.10\n",
      "- Sanrio Store: 85.70\n",
      "Total spent on stuffed animals and props: 20.50 + 25.60 + 45.10 + 85.70 = 176.90\n",
      "\n",
      "So, you spent $18.60 on restaurants, $48.00 on groceries, and $176.90 on stuffed animals and props.\n"
     ]
    }
   ],
   "source": [
    "response_small = mistral(prompt, model=\"mistral-small-latest\")\n",
    "print(response_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you categorize your spending. Here's the breakdown:\n",
      "\n",
      "1) Restaurants:\n",
      "   - McDonald's: $8.40\n",
      "   - Panda Express: $10.20\n",
      "   - World Food Wraps: $22.70\n",
      "   - Total for restaurants: $8.40 + $10.20 + $22.70 = $41.30\n",
      "\n",
      "2) Groceries:\n",
      "   - Safeway: $10.30\n",
      "   - Carrefour: $15.00\n",
      "   - Total for groceries: $10.30 + $15.00 = $25.30\n",
      "\n",
      "3) Stuffed animals and props:\n",
      "   - Toys R Us: $20.50\n",
      "   - Beanie Baby Outlet: $25.60\n",
      "   - Stuffed Animals Shop: $45.10\n",
      "   - Sanrio Store: $85.70\n",
      "   - Total for stuffed animals and props: $20.50 + $25.60 + $45.10 + $85.70 = $176.90\n",
      "\n",
      "So, you spent $41.30 on restaurants, $25.30 on groceries, and $176.90 on stuffed animals and props.\n"
     ]
    }
   ],
   "source": [
    "response_large = mistral(prompt, model=\"mistral-large-latest\")\n",
    "print(response_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing and checking code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"\n",
    "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "You can return the answer in any order.\n",
    "\n",
    "Your code should pass these tests:\n",
    "\n",
    "assert twoSum([2,7,11,15], 9) == [0,1]\n",
    "assert twoSum([3,2,4], 6) == [1,2]\n",
    "assert twoSum([3,3], 6) == [0,1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python solution for the problem:\n",
      "\n",
      "```python\n",
      "def twoSum(nums, target):\n",
      "    num_dict = {}\n",
      "    for i, num in enumerate(nums):\n",
      "        complement = target - num\n",
      "        if complement in num_dict:\n",
      "            return [num_dict[complement], i]\n",
      "        num_dict[num] = i\n",
      "    return []\n",
      "\n",
      "assert twoSum([2,7,11,15], 9) == [0,1]\n",
      "assert twoSum([3,2,4], 6) == [1,2]\n",
      "assert twoSum([3,3], 6) == [0,1]\n",
      "```\n",
      "\n",
      "In this solution, we first create an empty dictionary called `num_dict`. Then, we iterate through the `nums` list with enumerate to keep track of the current index `i` and number `num`.\n",
      "\n",
      "For each number, calculate its complement (i.e., `target - num`). If the complement is found in the `num_dict`, we have found the pair of numbers that sum up to the target, so we return their indices. Otherwise, add the current number and its index to the `num_dict`.\n",
      "\n",
      "Finally, if no pair is found, the function returns an empty list (though it's not necessary in this case, as the question states there is always exactly one solution).\n"
     ]
    }
   ],
   "source": [
    "print(mistral(user_message, model=\"mistral-large-latest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natively Fluent in English, French, Spanish, German, and Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\"\n",
    "Lequel est le plus lourd une livre de fer ou un kilogramme de plume\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une livre de fer pèse environ 0,453 kilogramme, tandis qu'un kilogramme de plumes pèse un kilogramme. Donc, un kilogramme de plumes est plus lourd qu'une livre de fer. Cependant, il est important de noter que le volume occupé par un kilogramme de plumes serait beaucoup plus grand que celui d'une livre de fer, car les plumes sont moins denses que le fer.\n"
     ]
    }
   ],
   "source": [
    "print(mistral(user_message, model=\"mistral-large-latest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: mistralai>=0.1.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (0.4.1)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (0.27.0)\n",
      "Requirement already satisfied: orjson<3.11,>=3.9.10 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (3.10.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.5.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (2.7.4)\n",
      "Requirement already satisfied: anyio in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25->mistralai>=0.1.2) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl (5.0 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.0.0 pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas \"mistralai>=0.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"transaction_id\": [\"T1001\", \"T1002\", \"T1003\", \"T1004\", \"T1005\"],\n",
    "    \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C002\", \"C001\"],\n",
    "    \"payment_amount\": [125.50, 89.99, 120.00, 54.30, 210.20],\n",
    "    \"payment_date\": [\n",
    "        \"2021-10-05\",\n",
    "        \"2021-10-06\",\n",
    "        \"2021-10-07\",\n",
    "        \"2021-10-05\",\n",
    "        \"2021-10-08\",\n",
    "    ],\n",
    "    \"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"],\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>payment_amount</th>\n",
       "      <th>payment_date</th>\n",
       "      <th>payment_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1001</td>\n",
       "      <td>C001</td>\n",
       "      <td>125.50</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1002</td>\n",
       "      <td>C002</td>\n",
       "      <td>89.99</td>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>Unpaid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1003</td>\n",
       "      <td>C003</td>\n",
       "      <td>120.00</td>\n",
       "      <td>2021-10-07</td>\n",
       "      <td>Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1004</td>\n",
       "      <td>C002</td>\n",
       "      <td>54.30</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>Paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1005</td>\n",
       "      <td>C001</td>\n",
       "      <td>210.20</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>Pending</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id customer_id  payment_amount payment_date payment_status\n",
       "0          T1001        C001          125.50   2021-10-05           Paid\n",
       "1          T1002        C002           89.99   2021-10-06         Unpaid\n",
       "2          T1003        C003          120.00   2021-10-07           Paid\n",
       "3          T1004        C002           54.30   2021-10-05           Paid\n",
       "4          T1005        C001          210.20   2021-10-08        Pending"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without funciton calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "    \"transaction_id\": [\"T1001\", \"T1002\", \"T1003\", \"T1004\", \"T1005\"],\n",
    "    \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C002\", \"C001\"],\n",
    "    \"payment_amount\": [125.50, 89.99, 120.00, 54.30, 210.20],\n",
    "    \"payment_date\": [\n",
    "        \"2021-10-05\",\n",
    "        \"2021-10-06\",\n",
    "        \"2021-10-07\",\n",
    "        \"2021-10-05\",\n",
    "        \"2021-10-08\",\n",
    "    ],\n",
    "    \"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"],\n",
    "}\n",
    "\"\"\"\n",
    "transaction_id = \"T1001\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following data, what is the payment status for \\\n",
    " transaction_id={transaction_id}?\n",
    "\n",
    "data:\n",
    "{data}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided data, the payment status for transaction_id T1001 is \"Paid\".\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. User: specify tools and query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_payment_status(df: data, transaction_id: str) -> str:\n",
    "    if transaction_id in df.transaction_id.values:\n",
    "        return json.dumps(\n",
    "            {\"status\": df[df.transaction_id == transaction_id].payment_status.item()}\n",
    "        )\n",
    "    return json.dumps({\"error\": \"transaction id not found.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\": \"Paid\"}\n"
     ]
    }
   ],
   "source": [
    "status = retrieve_payment_status(df, transaction_id=\"T1001\")\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_payment_date(df: data, transaction_id: str) -> str:\n",
    "    if transaction_id in df.transaction_id.values:\n",
    "        return json.dumps(\n",
    "            {\"date\": df[df.transaction_id == transaction_id].payment_date.item()}\n",
    "        )\n",
    "    return json.dumps({\"error\": \"transaction id not found.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\": \"2021-10-06\"}\n"
     ]
    }
   ],
   "source": [
    "date = retrieve_payment_date(df, transaction_id=\"T1002\")\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_payment_status = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"retrieve_payment_status\",\n",
    "        \"description\": \"Get payment status of a transaction\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"transaction_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The transaction id.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"transaction_id\"],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tool_payment_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_payment_date = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"retrieve_payment_date\",\n",
    "        \"description\": \"Get payment date of a transaction\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"transaction_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The transaction id.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"transaction_id\"],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tool_payment_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool_payment_status, tool_payment_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'retrieve_payment_status',\n",
       "   'description': 'Get payment status of a transaction',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'transaction_id': {'type': 'string',\n",
       "      'description': 'The transaction id.'}},\n",
       "    'required': ['transaction_id']}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'retrieve_payment_date',\n",
       "   'description': 'Get payment date of a transaction',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'transaction_id': {'type': 'string',\n",
       "      'description': 'The transaction id.'}},\n",
       "    'required': ['transaction_id']}}}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_functions = {\n",
    "    \"retrieve_payment_status\": functools.partial(retrieve_payment_status, df=df),\n",
    "    \"retrieve_payment_date\": functools.partial(retrieve_payment_date, df=df),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\": \"Paid\"}'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_to_functions[\"retrieve_payment_status\"](transaction_id=\"T1001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'retrieve_payment_status',\n",
       "   'description': 'Get payment status of a transaction',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'transaction_id': {'type': 'string',\n",
       "      'description': 'The transaction id.'}},\n",
       "    'required': ['transaction_id']}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'retrieve_payment_date',\n",
       "   'description': 'Get payment date of a transaction',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'transaction_id': {'type': 'string',\n",
       "      'description': 'The transaction id.'}},\n",
       "    'required': ['transaction_id']}}}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(role=\"user\", content=\"What's the status of my transaction?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Model: Generate function arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='f431b7d633f7448b9d3ec742bdf10b21', object='chat.completion', created=1719736113, model='mistral-large-latest', choices=[ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content='To find out the status of your transaction, I need the transaction ID. Please provide me with the transaction ID.', name=None, tool_calls=None, tool_call_id=None), finish_reason=<FinishReason.stop: 'stop'>)], usage=UsageInfo(prompt_tokens=161, total_tokens=184, completion_tokens=23))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat(\n",
    "    model=model, messages=chat_history, tools=tools, tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out the status of your transaction, I need the transaction ID. Please provide me with the transaction ID.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role='user', content=\"What's the status of my transaction?\", name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='assistant', content='To find out the status of your transaction, I need the transaction ID. Please provide me with the transaction ID.', name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='user', content='My transaction ID is T1001.', name=None, tool_calls=None, tool_call_id=None)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.append(\n",
    "    ChatMessage(role=\"assistant\", content=response.choices[0].message.content)\n",
    ")\n",
    "chat_history.append(ChatMessage(role=\"user\", content=\"My transaction ID is T1001.\"))\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat(\n",
    "    model=model, messages=chat_history, tools=tools, tool_choice=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='assistant' content='' name=None tool_calls=[ToolCall(id='fYw57YWGJ', type=<ToolType.function: 'function'>, function=FunctionCall(name='retrieve_payment_status', arguments='{\"transaction_id\": \"T1001\"}'))] tool_call_id=None\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. User: Execute function to obtain tool results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='retrieve_payment_status' arguments='{\"transaction_id\": \"T1001\"}'\n"
     ]
    }
   ],
   "source": [
    "tool_function = response.choices[0].message.tool_calls[0].function\n",
    "print(tool_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieve_payment_status'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_function.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"transaction_id\": \"T1001\"}'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_function.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transaction_id': 'T1001'}\n"
     ]
    }
   ],
   "source": [
    "args = json.loads(tool_function.arguments)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\": \"Paid\"}'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_result = names_to_functions[tool_function.name](**args)\n",
    "function_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_msg = ChatMessage(role=\"tool\", name=tool_function.name, content=function_result)\n",
    "chat_history.append(tool_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role='user', content=\"What's the status of my transaction?\", name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='assistant', content='To find out the status of your transaction, I need the transaction ID. Please provide me with the transaction ID.', name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='user', content='My transaction ID is T1001.', name=None, tool_calls=None, tool_call_id=None),\n",
       " ChatMessage(role='assistant', content='', name=None, tool_calls=[ToolCall(id='fYw57YWGJ', type=<ToolType.function: 'function'>, function=FunctionCall(name='retrieve_payment_status', arguments='{\"transaction_id\": \"T1001\"}'))], tool_call_id=None),\n",
       " ChatMessage(role='tool', content='{\"status\": \"Paid\"}', name='retrieve_payment_status', tool_calls=None, tool_call_id=None)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Model: Generate final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The status of your transaction T1001 is \"Paid\". Is there anything else I can assist you with?'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat(model=model, messages=chat_history)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RAG from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: mistralai>=0.1.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (0.4.1)\n",
      "Collecting numpy<2.0,>=1.0 (from faiss-cpu)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.25 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (0.27.0)\n",
      "Requirement already satisfied: orjson<3.11,>=3.9.10 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (3.10.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.5.2 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from mistralai>=0.1.2) (2.7.4)\n",
      "Requirement already satisfied: anyio in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai>=0.1.2) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25->mistralai>=0.1.2) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.5.2->mistralai>=0.1.2) (4.12.2)\n",
      "Downloading faiss_cpu-1.8.0.post1-cp312-cp312-macosx_11_0_arm64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Installing collected packages: numpy, faiss-cpu\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "Successfully installed faiss-cpu-1.8.0.post1 numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip3 install faiss-cpu \"mistralai>=0.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.deeplearning.ai/the-batch/issue-255/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anrilombard/Desktop/Hyperion/.venv/lib/python3.12/site-packages (from requests) (2024.6.2)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: urllib3, soupsieve, charset-normalizer, requests, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 charset-normalizer-3.3.2 requests-2.32.3 soupsieve-2.5 urllib3-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install requests bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear friends,On Monday, a number of large music labels sued AI music makers Suno and Udio for copyright infringement. Their lawsuit echoes The New York Times’ lawsuit against OpenAI in December. The question of what’s fair when it comes to AI software remains a difficult one. I spoke out in favor of OpenAI’s side in the earlier lawsuit. Humans can learn from online articles and use what they learn to produce novel works, so I’d like to be allowed to use AI to do so. Some people criticized my view as making an unjustifiable equivalence between humans and AI. This made me realize that people have at least two views of AI: I view AI as a tool we can use and direct to our own purposes, while some people see it as akin to a separate species, distinct from us, with its own goals and desires.If I’m allowed to build a house, I want to be allowed to use a hammer, saw, drill, or any other tool that might get the job done efficiently. If I’m allowed to read a webpage, I’d like to be allowed to read it with any web browser, and perhaps even have the browser modify the page’s formatting for accessibility. More generally, if we agree that humans are allowed to do certain things — such as read and synthesize information on the web — then my inclination is to let humans direct AI to automate this task. In contrast to this view of AI as a tool, if someone thinks humans and AI are akin to separate species, they’ll frame the question differently. Few people today think all species should have identical rights. If a mosquito annoys a human, the mosquito can be evicted (or worse). In this view, there’s no reason to think that, just because humans are allowed to do something, AI should be allowed to do it as well. To be clear, just as humans aren’t allowed to reproduce large parts of copyrighted works verbatim (or nearly verbatim) without permission, AI shouldn’t be allowed to do so either. The lawsuit against Suno and Udio points out that, when prompted in a particular way, these services can nearly reproduce pieces of copyrighted music.But here, too, there are complex issues. If someone were to use a public cloud to distribute online content in violation of copyright, typically the person who did that would be at fault, not the cloud company (so long as the company took reasonable precautions and didn’t enable copyright infringement deliberately). The plaintiffs in the lawsuit against Suno and Udio managed to write prompts that caused the systems to reproduce copyrighted work. But is this like someone managing to get a public cloud to scrape and distribute content in a way that violates copyright? Or is this — as OpenAI said — a rare bug that AI companies are working to eliminate? (Disclaimer: I’m not a lawyer and I’m not giving legal advice.)Humans and software systems use very different mechanisms for processing information. So in terms of what humans can do — and thus what I’d like to be allowed to use software to help me do — it’s helpful to consider the inputs and outputs. Specifically, if I’m allowed to listen to a lot of music and then compose a novel piece of music, I would like to be allowed to use AI to implement a similar input-to-output mapping. The process for implementing this mapping may be training a neural network on music that’s legally published on the open internet for people to enjoy without encumbrances.To acknowledge a weakness of my argument, just because humans are allowed to emit a few pounds of carbon dioxide per day simply by breathing doesn’t mean we should allow machines to emit massively more carbon dioxide without restrictions. Scale can change the nature of an act. When I was a high-school student in an internship job, I spent numerous hours photocopying, and I remember wishing I could automate that repetitive work. Humans do lots of valuable work, and AI, used as a tool to automate what we do, will create lots of value. I hope we can empower people to use tools to automate activities they’re allowed to do, and erect barriers to this only in extraordinary circumstances, when we have clear evidence that it creates more harm than benefit to society.Keep learning!Andrew A MESSAGE FROM DEEPLEARNING.AILearn to reduce the carbon footprints of your AI projects in “Carbon Aware Computing for GenAI Developers,” a new course built in collaboration with Google Cloud. Perform model training and inference jobs with cleaner, low-carbon energy and make your AI development greener! Join todayNewsU.S. to Probe AI Monopoly ConcernsU.S. antitrust regulators are preparing to investigate a trio of AI giants.What’s new: Two government agencies responsible for enforcing United States anti-monopoly laws agreed to investigate Microsoft, Nvidia, and OpenAI, The New York Times reported. How it works: The Department of Justice (DOJ) will investigate Nvidia, which dominates the market for chips that train and run neural networks. The Federal Trade Commission (FTC) will probe Microsoft and its relationship with OpenAI, which together control the distribution of OpenAI’s popular GPT-series models. In February, FTC chair Lina Khan said the agency would look for possible anti-competitive forces in the AI market. The DOJ is concerned that Nvidia may use unfair practices to maintain its market dominance. They may look into Nvidia’s CUDA software, which strengthens users’ reliance on its chips. They may also explore claims raised by French authorities that Nvidia favors some cloud computing firms over others.The FTC worries that the partnership between OpenAI and Microsoft, which owns 49 percent of OpenAI and holds a non-voting seat on OpenAI’s board of directors, may work to limit consumer choice. Microsoft’s April agreement with Inflection AI to hire most of its staff in return for a $650 million payment, which resembled an acquisition but left Inflection’s corporate structure intact, raised suspicions that the deal had been structured to avoid automatic antitrust scrutiny. The FTC previously investigated investments in Anthropic by Amazon and Google as well as whether OpenAI gathered training data in ways that harmed consumers. Behind the news: Government attention to top AI companies is rising worldwide. Microsoft’s partnership with OpenAI faces additional scrutiny by European Union regulators, who are probing whether the relationship violates EU regulations that govern corporate mergers. U.K. regulators are investigating Amazon’s relationship with Anthropic and Microsoft’s relationship with Mistral and Inflection AI. Last year, French regulators raided an Nvidia office over suspected anti-competitive practices. In 2022, Nvidia withdrew a bid to acquire chip designer Arm Holdings after the proposal attracted international regulatory scrutiny including an FTC lawsuit.Why it matters: Microsoft, Nvidia, and OpenAI have put tens of billions of dollars each into the AI market, and lawsuits, settlements, judgments, or other interventions could shape the fate of those investments. The FTC and DOJ similarly divided their jurisdictions in 2019, resulting in investigations into — and ongoing lawsuits against — Amazon, Apple, Google, and Meta for alleged anti-competitive practices in search, social media, and consumer electronics. Their inquiries into the AI market could have similar impacts. We’re thinking: Governments must limit unfair corporate behavior without stifling legitimate activities. Recently, in the U.S. and Europe, the pendulum has swung toward overly aggressive enforcement. For example, government opposition to Adobe’s purchase of Figma had a chilling effect on acquisitions that seems likely to hurt startups. The UK blocked Meta’s acquisition of Giphy, which didn’t seem especially anticompetitive. We appreciate antitrust regulators’ efforts to create a level playing field, and we hope they’ll take a balanced approach to antitrust.Chatbot for Minority LanguagesAn AI startup that aims to crack markets in southern Asia launched a multilingual competitor to GPT-4.What’s new: The company known as Two AI offers SUTRA, a low-cost language model built to be proficient in more than 30 languages, including underserved South Asian languages like Gujarati, Marathi, Tamil, and Telugu. The company also launched ChatSUTRA, a free-to-use web chatbot based on the model.How it works: SUTRA comprises two mixture-of-experts transformers: a concept model and an encoder-decoder for translation. A paper includes some technical details, but certain details and a description of how the system fits together are either absent or ambiguous. The concept model learned to predict the next token. The training dataset included publicly available datasets in a small number of languages for which abundant data is available, including English.Concurrently, the translation model learned to translate 100 million human- and machine-translated conversations among many languages. This model learned to map concepts to similar embeddings across all languages in the dataset. The authors combined the two models, so the translation model’s encoder fed the concept model, which in turn fed the translation model’s decoder, and further trained them together. More explicitly, during this stage of training and at inference, the translation model’s encoder receives text and produces an initial embedding. The concept model processes the embedding and delivers its output to the translation model’s decoder, which produces the resulting text. SUTRA is available via an API in versions that are designated Pro (highest-performing), Light (lowest-latency), and Online (internet-connected). SUTRA-Pro and SUTRA-Online cost $1 per 1 million tokens for input and output. SUTRA-Light costs $0.75 per 1 million tokens. Results: On multilingual MMLU (a machine-translated version of multiple-choice questions that cover a wide variety of disciplines), SUTRA outperformed GPT-4 in four of the 11 languages for which the developer reported the results: Gujarati, Marathi, Tamil, and Telugu. Moreover, SUTRA’s tokenizer is highly efficient, making the model fast and cost-effective. In key languages, it compares favorably to the tokenizer used with GPT-3.5 and GPT-4, and even narrowly outperforms GPT-4o’s improved tokenizer, according to Two AI’s tokenizer comparison space on HuggingFace. In languages such as Hindi and Korean that are written in non-Latin scripts and for which GPT-4 performs better on MMLU, SUTRA’s tokenizer generates less than half as many tokens as the one used with GPT-3.5 and GPT-4, and slightly fewer than GPT-4o’s tokenizer.Yes, but: Multilingual MMLU tests only 11 of SUTRA’s 33 languages, making it difficult to fully evaluate the model’s multilingual performance. Behind the news: Two AI was founded in 2021 by Pranav Mistry, former president and CEO of Samsung Technology & Advanced Research Labs. The startup has offices in California, South Korea, and India. In 2022, it raised $20 million in seed funding from Indian telecommunications firm Jio and South Korean internet firm Naver. Mistry aims to focus on predominantly non-English-speaking markets such as India, South Korea, Japan, and the Middle East, he told Analytics India.Why it matters: Many top models work in a variety of languages, but from a practical standpoint, multilingual models remain a frontier in natural language processing. Although SUTRA doesn’t match GPT-4 in all the languages reported, its low price and comparatively high performance may make it appealing in South Asian markets, especially rural areas where people are less likely to speak English. The languages in which SUTRA excels are spoken by tens of millions of people, and they’re the most widely spoken languages in their respective regions. Users in these places have yet to experience GPT-4-level performance in their native tongues.We’re thinking: Can a newcomer like Two AI compete with OpenAI? If SUTRA continues to improve, or if it can maintain its cost-effective service, it may yet carve out a niche.Conversing With the DepartedAdvances in video generation have spawned a market for lifelike avatars of deceased loved ones.What’s new: Several companies in China produce interactive videos that enable customers to chat with animated likenesses of dead friends and relatives, MIT Technology Review reported. How it works: Super Brain and Silicon Intelligence have built such models for several thousand customers. They provide a modern equivalent of portrait photos of deceased relatives and a vivid way to commune with ancestors.The developers use undisclosed tools to stitch photos, videos, audio recordings, and writings supplied by customers into interactive talking-head avatars of deceased loved ones.The cost has dropped dramatically. In December 2023, Super Brain charged between $1,400 and $2,800 for a basic chat avatar wrapped in a phone app. Today it charges between $700 and $1,400 and plans eventually to drop the price to around $140. Silicon Intelligence charges between several hundred dollars for a phone-based avatar to several thousand for one displayed on a tablet.Behind the news: The desire to interact with the dead in the form of an AI-generated avatar is neither new nor limited to China. In the U.S., the startup HereAfter AI builds chatbots that mimic the deceased based on interviews conducted while they were alive. Another startup, StoryFile, markets similar capabilities to elders (pitched by 93-year-old Star Trek star William Shatner) to keep their memory alive for younger family members. The chatbot app Replika began as a project by founder Eugenia Kuyda to virtually resurrect a friend who perished in a car accident in 2015. Yes, but: In China, language models struggle with the variety of dialects spoken by many elders.Why it matters: Virtual newscasters and influencers are increasingly visible on the web, but the technology has more poignant uses. People long to feel close to loved ones who are no longer present. AI can foster that sense of closeness and rapport, helping to fulfill a deep need to remember, honor, and consult the dead.We’re thinking: No doubt, virtual avatars of the dead can bring comfort to the bereaved. But they also bring the risk that providers might manipulate their customers’ emotional attachments for profit. We urge developers to focus on strengthening relationships among living family and friends.Benchmarks for Agentic BehaviorsTool use and planning are key behaviors in agentic workflows that enable large language models (LLMs) to execute complex sequences of steps. New benchmarks measure these capabilities in common workplace tasks. What’s new: Recent benchmarks gauge the ability of a large language model (LLM) to use external tools to manipulate corporate databases and to plan events such as travel and meetings. Tool use: Olly Styles, Sam Miller, and colleagues at Mindsdb, University of Warwick, and University of Glasgow proposed WorkBench, which tests an LLM’s ability to use 26 software tools to operate on five simulated workplace databases: email, calendar, web analytics, projects, and customer relationship management. Tools include deleting emails, looking up calendar events, creating graphs, and looking up tasks in a to-do list.The benchmark includes 690 problems that require using between zero to 12 tools to succeed. It evaluates individual examples based on whether the databases changed as expected after the final tool had been called (rather than simply whether particular tools were used, as in earlier work). In this way, a model can use tools in any sequence and/or revise its initial choices if they prove unproductive and still receive credit for responding correctly.Upon receiving a problem, models are given a list of all tools and an example of how to use each one. Following the ReAct prompting strategy, they’re asked first to reason about the problem and then use a tool. After they’ve received a tool’s output (typically either information or an error message), they’re asked to reason again and choose another tool. The cycle of reasoning, tool selection, and receiving output repeats until the model decides it doesn’t need to use another tool. The authors evaluated GPT-4, GPT-3.5, Claude 2, Llama2-70B, and Mixtral-8x7B. GPT-4 performed the best by a large margin: It modified the databases correctly 43 percent of the time. The closest competitor, Claude 2, modified the databases correctly 26 percent of the time.Planning: Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, and colleagues at Google published Natural Plan, a benchmark that evaluates an LLM’s ability to (i) plan trips, (ii) arrange a series of meeting times and locations, and (iii) schedule a group meeting. Each example has only one solution.The benchmark includes 1,600 prompts that ask the model to plan a trip based on an itinerary of cities, time to be spent in each city, total duration of the trip, days when other people are available to meet, and available flights between cities. 1,000 prompts ask the model to plan a schedule to meet as many people as possible. The prompts include places, times when people will be in each place, and how long it takes to drive from one place to another. 1,000 prompts ask the model, given the existing schedules of a number of people, to find a good time for them to meet.The authors tested GPT 3.5, GPT-4, GPT-4o, Gemini 1.5 Flash, and Gemini 1.5 Pro, using five-shot prompts (that is, providing five examples for context). Gemini 1.5 Pro achieved the highest scores on planning trips (34.8 percent) and scheduling group meetings (48.9 percent). GPT-4 ranked second for planning trips (31.1), and GPT-4o ranked second for scheduling meetings (43.7 percent). GPT-4 dominated in arranging meetings (47 percent), followed by GPT-40 (45.2 percent).Why it matters: When building agentic workflows, developers must decide on LLM choices, prompting strategies, sequencing of steps to be carried out, tool designs, single- versus multi-agent architectures, and so on. Good benchmarks can reveal which approaches work best.We're thinking: These tests have unambiguous right answers, so agent outputs can be evaluated automatically as correct or incorrect. We look forward to further work to evaluate agents that generate free text output.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "response = requests.get(\n",
    "    URL\n",
    ")\n",
    "html_doc = response.text\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "tag = soup.find(\"div\", re.compile(\"^prose--styled\"))\n",
    "text = tag.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as text file\n",
    "file_name = \"AI_monopolies_etc.txt\"\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 512\n",
    "chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(txt):\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks]).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 1024)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Make sure text_embeddings is a numpy array\n",
    "assert isinstance(text_embeddings, np.ndarray)\n",
    "\n",
    "# Add index to faiss index and cast text embedding to np.ndarray\n",
    "index = faiss.IndexFlatIP(text_embeddings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings_copy = np.array(text_embeddings, copy=True, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input not a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3q/r2zc74911hn2f967w656zszm0000gn/T/ipykernel_23036/630779001.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embeddings_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Hyperion/.venv/lib/python3.12/site-packages/faiss/class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Hyperion/.venv/lib/python3.12/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m  11449\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11450\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: input not a numpy array"
     ]
    }
   ],
   "source": [
    "index.add(text_embeddings_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the ways that AI can reduce emissions in Agriculture?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(question_embeddings, k=2)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "print(retrieved_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "\n",
    "def mistral(user_message, model=model, is_json=False):\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, messages=messages, response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "    else:\n",
    "        chat_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG notebook does not work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
